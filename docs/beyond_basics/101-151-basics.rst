.. _hirnibasics:

The Basic Hirni Workflow
========================

A ``hirni`` workflow comprises of two steps:

1. The generation of a *study dataset* with raw acquisition data and conversion-relevant
   metadata
2. The conversion of the raw data into a BIDS-compliant dataset

Ideally, the first step is an ongoing routine during the data acquisition phase of
the study: With each new participant scanned and tested, their data is imported
into a continuously growing study dataset. This section demonstrates this
continuous type of ``hirni`` workflow. But it is also possible to start
the first step only after all data is collected, and the next section
demonstrates this on a concrete example.
Once step one is accomplished, the second step is a single command, and the
result is a BIDS-compliant dataset with complete provenance reaching all the
way back to the raw data from the MR scanner.

During the conversion step, ``hirni`` relies completely on conversion-relevant
metadata and *specifications* for the conversion:
Which DICOM files constitute a NifTi file? Which modality is it?
What type of task was used? Shall the subject identifier be anonymized, or
shall a T1 weighted image file be defaced? Metadata and specifications are generated from
DICOM metadata or specified with arguments, but they can also be manually created,
extended, or edited. This semi-automatic process allows for the necessary
flexibility when converting raw data from complex acquisitions. It allows to
exclude failed acquisitions, fix errors that occurred during meta data curation
during scanning, specify whether anonymized or unanonimyzed subject identifiers
shall be used, and add steps such as custom defacing procedures to the conversion.

To understand the basic workflow, it can be helpful to have some basic
understanding of how imaging data looks like when it leaves an MRI scanner.
If you want to read up on this, take a look at the hidden section below.

.. findoutmore:: On some basics of DICOMs

   An MRI scanner will return the data it collected as DICOM (Digital Imaging
   and Communications in Medicine) files. DICOM  is an international
   standard to transmit, store, process, and display medical imaging information.
   It incorporates standards for many imaging modalities, among them magnetic
   resonance imaging (MRI), and includes protocols for image exchange, compression,
   and visualization.

   DICOM differs from other image formats in that it groups information into data sets,
   i.e., collections of information. A DICOM file consists of a *header* and
   *image data sets*. Usually, each DICOM file encodes a single 2D slice of the
   brain image, and what will later be a single 3D brain image is several dozen or
   even several hundred of individual DICOM files when it leaves the scanner.
   The information within the DICOM header is organized
   as a constant and standardized series of *tags* (two-number codes) that describe
   properties of the data. Depending on the tag, the associated data could either
   be provided by the scanner automatically, or filled in by the person that
   conducted the acquisition during scanning.
   By extracting data from these tags one can access important
   information regarding the participant's demographics, device, imaging sequence, and
   image specifics (e.g., dimensions of the image, slice thickness, or slice order).
   Apart from the actual image data, DICOM files therefore encompass also vast
   amounts of metadata.

   This structure and metadata, however, makes DICOM files also large and complex.
   Therefore, imaging data is transformed into simpler image formats that retain only
   a limited, relevant set of the images' metadata. The most common and widely
   adapted format for this is NifTi.
   It consists of fixed length header data with metadata about the
   image, and the image data. One NifTi file is an aggregation of several DICOM
   files, and its header data needs to be extracted or inferred from the correct
   DICOM tags. The NifTi header metadata contains at least the image matrix
   dimensions, the spatial resolution, the pixel depth, and the photometric
   interpretation, and enables software applications to recognize and correctly
   open the associated image.

   DICOMs are usually grouped into *acquisition*-wise tarballs. This means that the
   scanner produces and deposits one ``.tar.gz`` archive with all DICOMs that
   were acquired together into a scanner database, such as an
   `XNAT server <https://www.xnat.org/about/>`_. Depending on the length and type
   of data acquisition, such a tarball can contain thousands of DICOM files for
   a single subject. One acquisition usually consist of
   data from several *MRI sequences*: In a single acquisition, a typical MRI
   study collects anatomical images, and study-specific data such as task-based and/or
   resting state fMRI data,
   `diffusion-weighted MRI data <https://en.wikipedia.org/wiki/Diffusion_MRI>`_,
   `MR angiography <https://en.wikipedia.org/wiki/Magnetic_resonance_angiography>`_,
   or other `sequences <https://en.wikipedia.org/wiki/MRI_sequence>`_.
   All DICOM files that belong to the same sequence are called
   an *image series*. Thus, an acquisition tarball contains several image series
   that constitute data from different sequences, acquired in one session for a
   single subject. Usually, the DICOM identifier in the DICOM header contains
   information on which files belong to the same series and in which order.
   During conversion, each image series needs to be identified,
   sorted, and stacked into a NifTI file for the given sequence type.

Step 1: Creation of a study dataset
-----------------------------------

Once ``hirni`` is installed, we can create a standard dataset.
In the beginning, it is a regular DataLad dataset:

.. runrecord:: _examples/DL-101-151-101
   :language: console
   :workdir: dl-101

   $ datalad create mystudy

By installing ``hirni``, DataLad got access to a new :term:`run procedure` [#f1]_:
``cfg_hirni``. This procedure can create a study dataset skeleton inside of
``mystudy``, thus preparing ``mystudy`` to become the *study dataset*.

.. runrecord:: _examples/DL-101-151-102
   :language: console
   :workdir: dl-101

   $ cd mystudy
   $ datalad run-procedure cfg_hirni

Here is the layout of the fresh study dataset:

.. runrecord:: _examples/DL-101-151-103
   :language: console
   :workdir: dl-101/mystudy

   $ tree -L 3

The study dataset skeleton contains a ``README`` file and two ``.json`` files:

- ``studyspec.json``, which will contain some of the relevant specifications for
  conversion with ``hirni``, and
- `dataset_description.json <https://bids-specification.readthedocs.io/en/derivatives/03-modality-agnostic-files.html#dataset_descriptionjson>`_,
  a BIDS-compliant template file with fields to describe the dataset [#f2]_


Furthermore, ``cfg_hirni`` created a subdirectory ``code/`` and installed the
``hirni-toolbox`` inside of it.

.. findoutmore:: What's inside the toolbox?

   The ``cfg_hirni`` procedure populated the study dataset with a ``code/``
   directory. Inside, it installed a subdataset, ``hirni-toolbox``:

   .. runrecord:: _examples/DL-101-151-104
      :language: console
      :workdir: dl-101/mystudy

      $ datalad subdatasets

   The toolbox is ``hirni``\s collection of tools for the tasks you may want it
   to perform for you. For this, it references potentially relevant
   code and software for neuroimaging data.

   A collection of procedures in ``procedures/`` references
   scripts for operations such as conversion with `heudiconv <https://github.com/nipy/heudiconv>`_,
   defacing with `mridefacer <https://johnmuschelli.com/fslr/reference/mridefacer.html>`_,
   or quality control with `MRIQC <https://mriqc.readthedocs.io/en/stable/>`_.
   The potentially required software for these procedures (such as
   `FSL <https://fsl.fmrib.ox.ac.uk/fsl/fslwiki>`_, `heudiconv <https://github.com/nipy/heudiconv>`_,
   ...) is distributed alongside the procedures as Singularity images that can
   be retrieved automatically if the procedures that require them are used [#f3]_.

   You can find the toolbox dataset on GitHub at
   `github.com/psychoinformatics-de/hirni-toolbox <https://github.com/psychoinformatics-de/hirni-toolbox>`_
   if you want to take a closer look.

Initial dataset information
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let's say you have just started to prepare your neuroimaging study.
Even if you havn't acquired a single DICOM file yet, you can already curate
general metadata about your study into the study dataset. This helps to capture
information that you now know from the top of your head, but that you would
later need to dig through old documents for to find out about.
You can, however, also get to this step at any later point in time, or tackle it
if you are already sitting on a pile of acquired DICOM data, as will be demonstrated
in the upcoming section.

It is recommended to describe your study for humans in the ``README`` file, and fill
out missing values in ``dataset_description.json``. For the ``README``, you can
get creative. There is no format you need to adhere to. If you want, consider
using the handbooks DataLad dataset template found at :ref:`dataset_textblock`.
The contents of ``dataset_description.json`` currently are an empty template (note that
we pipe the output of the ``cat`` command to `jq <https://stedolan.github.io/jq/>`_,
a command line JSON processor that prettifies the output):

.. runrecord:: _examples/DL-101-151-105
   :language: console
   :workdir: dl-101/mystudy

   $ cat dataset_description.json | jq

Most of the fields will be straightforward to answer for you, and you should input
values for them into the empty strings or JSON lists the template provides.
You can either do this with an editor of your choice and save your changes with
:command:`datalad save` afterwards, or use ``hirni``\s webUI for assistance. If
you do it by hand and are not too familiar with JSON, be mindful to not create
invalid JSON by accidentally removing commas or forgetting about quotes!
There are helpful commandline
tools such as `jq <https://stedolan.github.io/jq/>`_ or webtools such as
`JSONlint <https://jsonlint.com/>`_ to help verify whether your files stayed valid.

.. findoutmore:: Using the webUI

   Let's take a quick look into how web assisted curation works. In order to fill
   out ``dataset_description.json``, start the web tool [#f4]_ with

   .. code-block:: bash

      $ datalad webapp --dataset . hirni

   You will be able to open ``http://127.0.0.1:5000/`` in a browser and see the
   following interface:

   .. figure:: ../artwork/src/hirni_sc1.png

   By clicking "Edit Study Metadata" you will be directed to a form with the
   same fields as ``dataset_description.json``. The text fields contain short
   help or example messages that you can replace. There is no need to fill in
   all fields at once -- leave fields with information that you don't yet have blank.
   You can return to this file later, and also edit existing contents.
   As soon as you are done filling out as much as you can or like, click the
   ``Save to dataset`` button at the end of the form.

   .. figure:: ../artwork/src/hirni_sc2.png

   Internally, this process writes your input to the ``dataset_description.json``
   file and concludes with a ``datalad save``, using the commit message
   ``"[HIRNI] Add specification snippet for: dataset_description.json"``

   There are two advantages to using the webUI: For one, it will write valid JSON.
   And secondly, as the webUI is only ever used by humans, data that is curated
   via this web tool has a label of confidence attached to it. This is not yet
   relevant for ``dataset_description.json``, but for editing ``studyspec.json``
   files later. In their JSON dictionaries, an ``"approved": "true"`` key will
   indicate if any information was given by a human and alter the webUI's
   handling of this information piece afterwards.

   Using the webUI for acquisition metadata will have a slightly more complex
   layout, but a separate findoutmore further down in this section will shed
   light on it.

Importing DICOMs
^^^^^^^^^^^^^^^^

As soon as the first acquisition exists, DICOM data can be imported into the study
dataset. A DICOM acquisition import takes the following structure in your study dataset:

.. code-block:: bash
   :emphasize-lines: 1-7

   ├── acquisition1             # subdirectory
   │   └── dicoms               # subdataset
   │       └──  ...
   │   ├── studyspec.json
   │   ├── protocol.pdf
   │   └── physio
   │       └──  ...
   ├── code
   │   └── hirni-toolbox
   │       ├── analyses
   │       ├── converters
   │       ├── data-retrieval
   │       ├── postprocessing
   │       └── procedures
   ├── dataset_description.json
   ├── README
   └── studyspec.json

Let's decipher that:

- There will be one subdirectory per acquisition tarball. In the above example,
  there is one acquisition, and it is represented in the subdirectory ``acquisition1``.
- Inside of this subdirectory, there will be at minimum a ``studyspec.json`` file
  with the specifications of this acquisition, and a ``dicoms/`` subdataset that
  contains the extracted DICOMs of the acquisition.
- Optionally, other study relevant data such as the
  acquisition protocol or physiological data can be added.

.. index:: ! datalad command; hirni-import-dcm

This structure is automatically created if ``hirni`` is pointed to the DICOM
tarball with the :command:`datalad hirni-import-dcm`
command. This command needs a path or url to a DICOM tarball of one acquisition,
and an acquisition ID that will become the subdirectory name. In the file hierarchy
above, this acquisition ID was ``acquisition1``.

The only purpose of importing DICOMs into the study dataset at this point is
to extract their conversion-relevant metadata: Which DICOMs constitute an image
series, what subject was scanned, which task was performed? A lot of meta data
can be automatically extracted, but in addition to the DICOM
tarball the command can also take specifications for such metadata as
optional command line arguments. This is helpful if DICOMs don't have comprehensive
header information, if it is information that is not part of the DICOM header
(such as (potentially) the subject ID, and certainly the anonymized subject ID),
or if it is non-standard metadata, such as newly added BIDS terms that would
not be detected automatically by ``hirni`` (see the hidden section on rules to
find out how to teach ``hirni`` about any DICOM metadata though). To curate
metadata during import from the command line, you can specify the subject identifier as used
during scanning with the ``--subject`` option, and an anonymized ID (such as ``001``)
that this subject should be identified with in the final BIDS dataset
with the ``--anon-subject`` option. If there are other BIDS-relevant metadata
terms about this acquisition, you can include them with the ``--properties``
option and a JSON dictionary as in this example:
``--properties '{"bids-task": "oneback"}'``

.. note::

   *Some* of this information can be inferred from DICOM headers if
   it was correctly filled in during scanning, for example the acquisition ID and
   the subject identifier. In principle, any information from the DICOM headers
   can be extracted automatically, but it might require custom *rules*. A later
   findoutmore will elaborate on rules and how to customize them.

To import the first acquisition of the first subject, you run a command similar
to this one inside of your study dataset::

   $ datalad hirni-import-dcm --anon-subject 001 path/to/DICOMs.tar.gz acquisition1

Throughout your data acquisition, or -- if everything is already acquired -- in
one go afterwards, you import all acquisitions of the study into your study
dataset.


Semi-automatic metadata extraction and conversion specification
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

Ultimately, the :command:`datalad hirni-import-dcm` serves only one goal:
Collect all relevant information from the DICOM headers to enable the conversion
from DICOM to NIfTI with the underlying tools ``hirni`` uses: ``heudiconv``,
which itself is using `dcm2niix <https://github.com/rordenlab/dcm2niix>`_ internally.

.. findoutmore:: Underlying tools for DICOM conversions

   ``hirni`` uses established tools under the hood to perform the conversion from
   DICOM to BIDS-compliant NIfTI files. For this, it relies on
   `heudiconv <https://heudiconv.readthedocs.io/en/latest/?badge=latest>`__, which exploits
   `dcm2niix <https://github.com/rordenlab/dcm2niix>`_ for the data conversion,
   and allows organizing data into structured directory layouts such as BIDS.
   ``hirni``\s aim is to ease and automate the use of these tools more.

   **dcm2niix**: ``dcm2niix`` is an established and fast command line tool to
   convert DICOM to NIfTI.
   A typical dcm2niix call points the software to a directory with DICOM images,
   specifies an output directory, and a file name for the output file. The software
   can generate associated ``json`` files with additional scan information such
   as slice timing that are relevant for BIDS compliance.

   **heudiconv**: ``heudiconv`` is a tool to convert and structure DICOM data.
   It uses ``dcm2niix`` for conversion, adds additional functionality for
   heuristics-based directory layouts and naming schemes with BIDS assistance,
   and integrates with DataLad. Running ``heudiconv`` on DICOM images can produce
   a DataLad dataset with NifTI files in BIDS compliant directory structures and
   their BIDS-relevant DICOM-metadata in accompanying ``json`` files, if done correctly.
   Using ``heudiconv`` involves two steps:
   Firstly, generating *heuristics* that define file and directory name templates for
   DICOM image series (a semi-automatic process that involves editing generated
   Python scripts), and secondly, for each subject and session, converting
   the DICOMs specified in the heuristic into NifTI files following naming
   conventions also specified in the heuristics. A short tutorial can be found
   `here <https://nipy.org/heudiconv>`_.

   Ultimately, ``hirni`` will use the DICOM metadata in each acquisition's
   ``studyspec.json`` file to call ``heudiconv`` for each subject and session
   with appropriate heuristics in a single conversion call. It is not necessary
   to know ``heudiconv`` to understand how ``hirni`` works, but knowing
   ``heudiconv`` can help to see how ``hirni`` makes conversions from raw to
   BIDS data easier.


This information is stored as aggregated metadata from the subdataset
in ``.datalad/metadata`` [#f5]_, and as acquisition-wise specifications in
``<acquisition-ID>/studyspec.json``.
You can find out more about the details of this process below.

.. findoutmore:: Internals of this command

   :command:`datalad hirni-import-dcm` executes a number of commands in order to
   prepare the conversion of the study dataset.

   Ultimately, DICOMs are only extracted from the tarball archive to access and
   query their headers' metadata. For some of this metadata extraction, the ``dicom``
   :term:`extractor` of ``datalad-neuroimaging`` is used. Together with
   ``datalad-metalad``, this extractor can record metadata from standard DICOM files,
   and, among other things, group together DICOM files that belong to an image series,
   i.e., a single sequence within the acquisition.
   The resulting metadata is *aggregated* into the study dataset [#f5]_.

   Beyond that, ``hirni``\s own metadata command :command:`hirni-dicom2spec` is
   executed and prepopulated an acquisition-specific ``studyspec.json`` file
   inside of the acquisition subdirectory. This file contains specifications
   for later structuring and naming
   of converted data to BIDS-compliant structures, such as the task name.
   Unlike the metadata mentioned above, metadata in ``studyspec.json`` files can and
   should be manually edited to insert custom or yet missing conversion-relevant
   information later. During DICOM import this file is prepopulated with relevant
   fields, and prefills fields with available metadata where available,
   for example about the location and version of the files and any BIDS-relevant
   information that is available from the DICOM headers.

   All of this metadata extraction is therefore crucial for the upcoming conversion,
   and the study dataset will receive a large chunk of the DICOM metadata
   required for the conversion directly at the time of the DICOM import.

   .. findoutmore:: Beyond metadata: Disk space savings

      The basis of the import and data handling is a :command:`datalad add-archive-content`
      (:manpage:`datalad-add-archive-content` manual) command. This command -- together
      with the way the data import is performed -- helps to save disk space in the
      study dataset: The ``tar.gz`` archive is saved in its compressed form into a
      :term:`branch` of the subdataset, and is extracted into the ``master`` branch
      of the dataset. From the DICOM files in ``master``, metadata can be extracted.
      With :command:`datalad add-archive-content`, the archive in the dataset branch
      is treated like a :term:`special remote`: On demand, individual files from
      ``master`` could be re-retrieved from the tarball. Therefore, the archive in
      the separate branch is essentially a compressed DICOM storage.
      Because this ensures one available source for the data, the (larger) file
      content of the extracted DICOMs in ``master`` can be dropped automatically
      right after metadata aggregation. Thus, instead of large, extracted DICOMs,
      the subdatasets only contains the compressed tarball, while ensuring that
      DICOM files can nevertheless be re-obtained from the archive on demand.

However, not all relevant information can be inferred from the DICOM headers.
Sometimes, DICOM headers contain false information, too, or acquisitions contain
image series from failed MRI sequences that should not be converted. And while
``hirni``\s conversion *can* incorporate special procedures such as anonymizing
subject identifiers or defacing anatomical images, it still needs a human decision
to do so. For these and similar cases that require human input, manual
specifications are necessary. The next few paragraphs and findoutmores in this
section will elaborate on the details of specifications, and demonstrate how they
can be edited.

Specifications
""""""""""""""

During DICOM import, the command ``hirni-dicom2spec`` generated so called
*specification snippets* from DICOM metadata and stored them in the
``studyspec.json`` *specification* file. Specifications describe *data entities*
of the study dataset. An entity is not necessarily a single file or directory,
but any study dataset content that is one logical unit that should be processed
in the same way, such as a DICOM series (which likely is a subset of the data in
``dicoms/``), all respiratory traces inside of a ``physio/`` subdirectory, or a
single ``events.txt`` stimulation log file for the acquisition.

Ultimately, the specification file will store all relevant information for the
conversion process for the given acquisition.
At this point in time, after the DICOM import, it contains
only automatically extracted DICOM related metadata, and manually specified
DICOM metadata such as anonymized subject IDs. Just prior to the conversion,
however, the ``studyspec.json`` will contain all relevant metadata on all relevant
files in the acquisition subdirectory as a collection of specification snippets.

Put into simple terms, for each data entity, a specification snippet in the specification file defines how it should be
converted (in other words: treated). Note that "conversion" does not exclusively
refer to a DICOM to NIfTI conversion -- in ``hirni``\s terms, a conversion is
any action that is undertaken with an entity in the study dataset to get it into
the form it should take in the BIDS dataset. This could be a simple action
such as copying or renaming a file, but also complex, multistepped processes,
for example defacing, or, in the case of DICOM series, conversion to NIfTI files.

In more technical terms, specification snippets define a list of
:term:`run procedure`\s and how exactly they are called. There is one run-procedure
that handles defacing, for example, and the specification snippet merely matches
the procedure to be performed to the data entity it should run on (and potentially
specify additional options). If you don't know what a procedure is, yet, please
give section :ref:`procedures` a quick read to get an initial overview. The upcoming
paragraph on procedures will elaborate a bit more, as well.
In addition to *procedures*, specification snippets also contain fields to define
potentially relevant BIDS terms about the data. If you are not familiar with the general
naming scheme in BIDS, please head over to `bids.neuroimaging.io <https://bids.neuroimaging.io/>`_
and take a look at the basic logic behind file names.

Specifications, therefore, are fundamental to ``hirni`` and indispensable for a successful
conversion. But they are also quite hard to get a good grasp on and look confusing
when you open them up in an editor instead of in the webUI. If you want
to get a detailed picture, this findoutmore will shed some light on them.

.. findoutmore:: On specifications

   Reading, understanding, and creating specification snippets requires some
   pre-existing knowledge on certain topics, and an understanding of the data
   and its properties.
   Specifications are defined in ``studyspec.json`` files. These files contain
   specification snippets as one line of JSON stream.

   .. findoutmore:: Some fundamentals of JSON and JSON stream

      JSON (JavaScript Object Notation) is easy to read when it is formatted
      nicely (for example with `jq <https://stedolan.github.io/jq/>`_): In JSON,
      data is stored in ``key-value`` pairs (such as ``{"name":"Joanna"}``), and
      separated by commas. Curly braces (``{}``)
      hold *objects* (dictionaries), such as

      .. code-block:: JSON

         {
          "person": {
             "name": "Joanna",
             "role": "data scientist"
             }
          }

      and square brackets (``[]``) hold *arrays* (lists), as in

      .. code-block:: JSON

         {
          "employees":[
           {
            "name": "Joanna",
            "role": "data scientist"
            },
            {
             "name": "Anna",
             "role": "software developer"
            },
            {
              "name": "Peter",
              "role": "marketing analyst"
            }
           ]
         }

      JSON has the inconvenient side effect that a single mistake in the document
      (e.g., a missing comma, or misplaced bracket) invalidates the whole document.
      Therefore, specifications are written in JSON streaming format.
      In JSON stream, the JSON essentially contains fewer linebreaks. Each
      line contains one valid JSON dictionary. This format allows per-line processing,
      and guarantees that all valid lines can be processed, even if the document
      contains some invalid lines. Here is an example of one specification snippet
      (this one belongs to the DICOMs of an acquisition). It is represented as
      a nested JSON dictionary and is one long line of text::

         $ {"anon-subject":{"approved":false,"value":"001"},"bids-acquisition":{"approved":false,"value":null},"bids-contrast-enhancement":{"approved":false,"value":null},"bids-direction":{"approved":false,"value":null},"bids-echo":{"approved":false,"value":null},"bids-modality":{"approved":false,"value":"bold"},"bids-reconstruction-algorithm":{"approved":false,"value":null},"bids-run":{"approved":false,"value":"01"},"bids-session":{"approved":false,"value":null},"bids-task":{"approved":false,"value":"oneback"},"comment":{"approved":false,"value":""},"dataset-id":"2f2a44d8-7271-11ea-861d-f9fd9dd57046","dataset-refcommit":"64644f46a471424df817dd5001f72b223c6f7a33","description":{"approved":false,"value":"func_task-oneback_run-1"},"id":{"approved":false,"value":401},"location":"dicoms","procedures":[{"on-anonymize":{"approved":false,"value":false},"procedure-call":{"approved":false,"value":null},"procedure-name":{"approved":false,"value":"hirni-dicom-converter"}}],"subject":{"approved":false,"value":"02"},"type":"dicomseries:all"}

      When processed, conversions are executed in the order of the specification
      snippets. But worry not -- it is only in advanced use cases that you would
      write a snippet by hand.

   Below, you can see the a single specification
   snippet, formatted into JSON for better readability. It is a snippet that would
   be generated automatically during DICOM import for one DICOM series:

   .. findoutmore:: see the formatted snippet

      .. code-block:: JSON

         {
          "anon-subject": {
            "approved": false,
            "value": "001"
          },
          "bids-acquisition": {
            "approved": false,
            "value": null
          },
          "bids-contrast-enhancement": {
            "approved": false,
            "value": null
          },
          "bids-direction": {
            "approved": false,
            "value": null
          },
          "bids-echo": {
            "approved": false,
            "value": null
          },
          "bids-modality": {
            "approved": false,
            "value": "bold"
          },
          "bids-reconstruction-algorithm": {
            "approved": false,
            "value": null
          },
          "bids-run": {
            "approved": false,
            "value": "01"
          },
          "bids-session": {
            "approved": false,
            "value": null
          },
          "bids-task": {
            "approved": false,
            "value": "oneback"
          },
          "comment": {
            "approved": false,
            "value": ""
          },
          "dataset-id": "2f2a44d8-7271-11ea-861d-f9fd9dd57046",
          "dataset-refcommit": "64644f46a471424df817dd5001f72b223c6f7a33",
          "description": {
            "approved": false,
            "value": "func_task-oneback_run-1"
          },
          "id": {
            "approved": false,
            "value": 401
          },
          "location": "dicoms",
          "procedures": [
            {
              "on-anonymize": {
                "approved": false,
                "value": false
              },
              "procedure-call": {
                "approved": false,
                "value": null
              },
              "procedure-name": {
                "approved": false,
                "value": "hirni-dicom-converter"
              }
            }
          ],
          "subject": {
            "approved": false,
            "value": "02"
          },
          "type": "dicomseries:all"
          "uid": "1.2.0260.1.3680043.2.1143.53213523436472938475829384762332",
        }

   This snipppet consists of a number of nested dictionaries. A few are potentially
   relevant BIDS terms, such as ``bids-acquisition``, or ``bids-run``. Not all of
   them will apply to your study, but they are in the template to allow their use
   should they be relevant. A few of them are pre-filled with values that were
   derived from DICOM headers, such as

   .. code-block::

       [...]
       "bids-modality": {
         "approved": false,
         "value": "bold"
       },
       "bids-run": {
         "approved": false,
         "value": "01"
       },
       "bids-task": {
         "approved": false,
         "value": "oneback"
       },
       [...]

   Another dictionary is the ``anon-subject`` object. It contains the value
   provided with the :command:`hirni-import-dcm` call in the ``--anon-subject``
   flag. There also is a ``subject`` dictionary. Its value was either specified
   in :command:`hirni-import-dcm` or derived from DICOM headers. The information
   in these dictionaries will be used to create a *virtual* BIDS term ``bids-subject``
   depending on whether subject names should be anonymized. If the
   ``--anonymize`` flag is set during conversion, the ``anon-subject`` ID will
   be used in file names instead of the ``subject`` ID.

   .. code-block::

      "anon-subject": {
        "approved": false,
        "value": "001"
      },
     [...]
      "subject": {
        "approved": false,
        "value": "02"
      },


   Three name-value pairs define the dataset ID of the data entity the
   snippet refers to, a path to the entity relative to the location of the
   specfile, and the latest commit (the ``refcommit``).
   This serves to capture the provenance of the input data and its version:

   .. code-block::

      "dataset-id": "2f2a44d8-7271-11ea-861d-f9fd9dd57046",
      "dataset-refcommit": "64644f46a471424df817dd5001f72b223c6f7a33",
      [...]
      "location": "dicoms",

   The key ``uid`` and ``type`` are two different means to refer to the data
   entity. For DICOM series, the ``uid`` is a unique identifier of the DICOM
   series. The type key is a "category" indication for the given data entity.
   It has a special purpose for DICOM conversion and should not be changed for
   DICOM entities, but it can be a helpful trick in all other data entity
   instances.

   .. findoutmore:: Utilizing the type key

       With the exception of DICOM data entities, a snippet's ``type`` key contains
       an arbitrary label (by default: ``generic file``).
       This label can be used as a "category" to group related
       specification snippets together. For example, all stimulation log files could
       get the ``type`` label "events". During the conversion step, using command
       line options, conversion can be limited to only specification snippets of a
       particular ``type`` label. This can be useful to test whether conversion routines
       work on a subset of the data, for example, or to simply convert only some parts
       of a study dataset.

   .. todo::

      - what is the type key for DICOMs?
      - uid for dicoms is the series identifier NOT CLEAR YET - VIDEO IS INCONSISTENT.
      - The conversion with heudiconv acts on a complete dicom subdataset, i.e.
        potentially on several image series at once (HOW DOES THIS CHANGE WITH NEW
        FEATURES?)

   .. code-block::

      "type": "dicomseries:all"
      "uid": "1.2.0260.1.3680043.2.1143.53213523436472938475829384762332",

   Finally, a list holds a number of dictionaries that define the procedure(s) that
   are to be performed on the DICOMs when this snippet is processed and how those
   procedures are to be executed. This is how it looks like by default for a
   DICOM series:

   .. code-block::

      "procedures": [
       {
         "on-anonymize": {
           "approved": false,
           "value": false
         },
         "procedure-call": {
           "approved": false,
           "value": null
         },
         "procedure-name": {
           "approved": false,
           "value": "hirni-dicom-converter"
         }
       }
      ],

   One of the dictionaries is the procedure name (``procedure-name``), in this
   case the ``hirni-dicom-converter`` that is part of toolbox and exists under
   ``code/toolbox/procedures``. Another
   dictionary is the procedure call (``procedure-call``), which defines how the
   procedure should be called -- this can be preconfigured, set to a default, or
   edited manually. For ``hirni-dicom-converter``, no specific call format is
   necessary. For some data, the ``on-anonymize``
   dictionary is included as a switch that determines whether the procedure runs
   during conversion if ``--anonymize`` is specified (???? IS THIS CORRECT?).

   You may have noticed that each dictionary has a boolean ``approve`` field.
   This exists for the webUI:
   By default, all approve fields are false and thus editable from the webUI.
   Any value can be marked as "approved" in the webUI, however, which prevents
   the value from being edited further.

   .. todo::

      - the approved key is added automatically by hirni depending on what call method
        returns. aproved is false by default. webui can distinguish false and true
        values. all false will show the fields as editable and asks for approval, all
        trues
      - no procedure definition in dicom2spec

It is important to check and potentially edit the specification that were derived
during the DICOM import. For this, just as for the dataset description, the
WebUI is useful.

.. findoutmore:: Editing acquisition metadata with the WebUI

   In order to use the WebUI to edit study metadata, start it with the same
   command as used before::

      $ datalad webapp -d . hirni

   This time, select the field "Edit acquisition metadata", and select the
   acquisition you want to edit or check from the drop-down menu.

   **General overview:**
   The WebUI now displays a more complex layout. There is one field for
   each JSON dictionary in the specification snippet of the DICOM series.
   It allows adding or editing descriptions or deleting fields all together,
   and it allows to tick an information to mark it as "approved".
   Apart from metadata fields that describe the nature of the data, there also
   is a large input field for procedures for each DICOM series.


   **Editing or adding BIDS relevant metadata:**

   .. todo::

       - difference betwen dicomseries:all and dicom series?
       - what does delete do? when should I use it?

   **Selecting procedures:**
   Adding procedures, for example defacing, can be done by selecting an
   available procedure from the drop-down menu. This menu will list all available
   procedures on your system, and those added by hirni are usually identified
   with a ``hirni-`` prefix (apart from the ``<copy|move>-converter`` procedures,
   and the ``fslroi`` procedure).

  .. todo::

     - procedure call: do all procedures have default calls?

Procedures
""""""""""

The fundamental conversion mechanism relies on DataLad's :term:`run procedure`\s.
Each specification snippets does not only define metadata about the data
entity, but also how this data entity shall be treated during conversion -- from
simple operations such as "copying and renaming" up to complex procedures
such as defacing. The findoutmore above already showed some example procedure
definitions, but there is more information about procedures to be known:

The available procedures are the following:

- ``copy-converter``: Copy a file
- ``hirni-dicom-converter``: Convert DICOMs to NifTis (automatically selected for
  all DICOM series)
- ``hirni_mridefacer``: Deface images during the conversion
- ``hirni-physiobox-converter``: TODO
- ``move-converter``: TODO rename a file?
- ``fslroi``: TODO



.. findoutmore:: Formatting procedure calls

   .. todo::

      - replacements: script = procedure call, ds = dataset it is called upon,
        and all other keys in the given snippet in double curly brackets

      - procedures: Should all use datalad run or datalad containers-run

.. findoutmore:: Custom rules for DICOM metadata extraction

   Rules are configurations that apply settings that teach ``hirni``\s
   :command:`hirni-dicom2spec` command about *custom* DICOM header fields to extract
   and store inside of ``studyspec.json`` files. By creating custom rules, automatic
   metadata extraction for these fields is triggered whenever a
   :command:`hirni-dicom2spec` or :command:`hirni-import-dcm` command is run.
   This allows to add custom metadata from DICOM headers to the specification
   files.

   .. note::

      Please be aware that the metadata extraction does not rely on a new
      :command:`hirni-import-dcm` command. Let's say you imported DICOMs, and
      created custom rules afterwards. It is not necessary to re-import the
      DICOMs -- simply run a single :command:`hirni-import-dcm` call (shown in
      detail later in this findoutmore). Do also note that after one initial
      :command:`hirni-import-dcm` command, no actual DICOM content is needed.
      :command:`hirni-dicom2spec` acts solely upon the extracted DICOM metadata.

   Existing rules for hirni can for example infer the subject name or the order
   of images in a DICOM series, and whatever rules extract is written to the
   ``studyspec.json`` file of the acquisition. Because :command:`hirni-dicom2spec`
   only deals with DICOM metadata, all information that this command extracts
   will be given to `HeuDiConv <https://github.com/nipy/heudiconv>`_ for
   DICOM conversion.

   .. todo::

      find out how heuristics for heudiconv are derived from this

   Rules are implemented as `Python classes <https://docs.python.org/3/tutorial/classes.html>`_.
   In order to write your own set of
   rules, you will need to create a Python file that contains them. In order to
   apply the rules, you need to add them -- just like the configurations
   introduced in chapter :ref:`chapter_config` -- with :command:`git config` on
   a scope level of your choice.

   **Writing rules:**

   .. note::

      Writing rules is a very advanced aspect of using ``hirni`` and should
      not be a standard step in the workflow. It can be useful if you want to
      achieve custom DICOM conversions, but it requires some proficiency with
      Python and experience with all underlying tools.

   In order to write a rule, write a Python class with the help of the
   template file that ``hirni`` provides
   `in its source code <https://github.com/psychoinformatics-de/datalad-hirni/blob/master/datalad_hirni/resources/rules/custom_rules_template.py>`_.
   Each rule is a class with at least a *constructor* (the ``__init__``) and
   a *call* method:

   .. code-block:: Python

      class MyDICOM2SpecRules(object):

          def __init__(self, dicommetadata):
              """
              Parameter
              ----------
              dicommetadata: list of dict
                  dicom metadata as extracted by datalad; one dict per image series
              """
              self._dicom_series = dicommetadata

          def __call__(self, subject=None, anon_subject=None, session=None):
              """
              Parameters
              ----------
              Returns
              -------
              list of tuple (dict, bool)
              """
              spec_dicts = []
              for dicom_dict in self._dicom_series:
                  spec_dicts.append((self._rules(dicom_dict,
                                                 subject=subject,
                                                 anon_subject=anon_subject,
                                                 session=session),
                                     self.series_is_valid(dicom_dict)
                                     )
                                    )
              return spec_dicts

          def _rules(self, series_dict, subject=None, anon_subject=None,
                     session=None):

              return {'description': series_dict['SeriesDescription']
                      if "SeriesDescription" in series_dict else '',

                      'comment': 'I actually have no clue',
                      'subject': series_dict['PatientID'] if not subject else subject,
                      'anon-subject': anon_subject if anon_subject else None,
                      'bids-session': session if session else None
                      }

          def series_is_valid(self, series_dict):

              return series_dict['ProtocolName'] != 'ExamCard'


      __datalad_hirni_rules = MyDICOM2SpecRules

   Usually, you don't need to touch the constructor. In it, the extracted metadata
   from the entire DICOM acquisition (``dicommetadata``) is automatically made
   available to the class.
   Because it is now a class attribute (``self._dicom_series``), this metadata
   can be queried in the ``call`` method. During execution, the newly generated
   rules will be applied to all DICOM series in the acquisition, and each of these
   calls will create one specification snippet in the ``studyspec.json`` file.

   ``dicommetadata`` is a list of dictionaries with extracted DICOM header fields
   that can be queried with standard Python operations (as shown in ``_rules()``).
   Any number of keys is possible.
   What the call method needs to return is a list of dictionaries with metadata that
   will be written into the ``studyspec.json`` file of the acquisition (``spec_dicts``)
   per DICOM series.

   The small function ``series_is_valid`` is used to determine whether a DICOM
   series is valid. Note that this needs a manufacturer-specific approach. For
   Siemens scanners, a series is invalid if the DICOM header field ``ProtocolName``
   is ``ExamCard``. If this happens, the rule template above would mark the
   series as invalid, but this function needs to be adjusted or removed, depending
   on the scanner type used for your acquisitions. To get a hang on querying
   metadata and writing rules, it can be helpful to use debugging tools such
   as `pdb breakpoints <https://docs.python.org/3/library/pdb.html#pdb.set_trace>`_
   when you start implementing your rule.

   Because a single Python script can contain several classes, the
   attribute ``__datalad_hirni_rules`` at the end of the file needs to be given
   all classes that contain relevant rules.

   **Adding rules:**
   Once custom rules are written, ``hirni`` needs to know where the scripts for
   these rules lie. This is done via :command:`git config`. The key of the
   configuration is  ``datalad.hirni.dicom2spec.rule`` and its value is an absolute
   path to the Python file containing the rules::

      $ git config --local --add datalad.hirni.dicom2spec.rule "/home/me/myrules.py"

   Rules can be set for different scopes (applying on a system-, user-, or
   dataset-level), and if a configuration exists on two or more levels with
   different values, more specific scopes take precedence over more general scopes
   [#f6]_.

   In order to extract metadata with newly added rules (if :command:`hirni-import-dcm`
   was run once at any point before) one can call :command:`hirni-dicom2spec`
   like this::

      $ datalad hirni-dicom2spec -s <path/to/studyspec.json> <path/to/dicoms>

   If you are in one acquisition subdirectory, this would work::

      $ datalad hirni-dicom2spec -s studyspec.json .

   .. note::

      Do note that upon re-invocing :command:`hirni-dicom2spec` the existing
      specification snippets in ``studyspec.json`` will be overwritten! Therefore,
      don't do costly editing with the webUI if you intend to run
      :command:`hirni-dicom2spec` again.



Adding other data
^^^^^^^^^^^^^^^^^

Neuroimaging studies usually encompass more than only imaging data. Depending
on the study, there may be measures from additional modalities (such as
physiological or behavioral data), stimulation protocols, acquisition protocols
from the scanner, or other digital data.

As it is part of each acquisition, this additional data should be added into
each acquisition subdirectory.
In order to add additional data, simply copy it into the acquisition subdirectory
(not into the ``dicoms/`` subdataset!). Afterwards, save the addition with
:command:`datalad save` and a helpful commit message.
It does not need to follow any particular
structure, but in order to be part of the conversion (i.e., added to the
BIDS-compliant dataset in the correct subject and session directories, named
correctly and BIDS-compliant, and potentially processed in a file-specific
way), it needs to get a specification.
There are commands to *help* with that, but there is no fully automated
specification derivation for additional data. Here is a sketch of how the
process looks like:

.. todo::

   Sketch how to do a specification


Anonymization
"""""""""""""

.. todo::

   - list which procedures are available and what they do
   - foreshadow how they can be called

.. findoutmore:: Why anonymizing measures should be done during the conversion

   Basic ethical practices in science and the General Data Protection Regulation
   (GDPR) of the European Union require that potentially identifying information
   of subjects' data needs to be anonymized to protect the identity and privacy
   rights of research participants.

   MRI data contains a lot of identifiable data. DICOM headers can contain names
   or birth dates of participants, and even subject identifiers can allow
   guestimates about the participants.
   Actual raw DICOM data from neuroimaging studies also contains faces -- a
   *very* identifying piece of information about a person. None of this information
   is therefore allowed be distributed to people that did not conduct the original
   study: In order to comply to ethical and privacy requirements, datasets with
   neuroimaging data need to be anonymized. Usually, this requires anonymous
   subject identifiers (ascending zero-padded integers for simultaneous
   BIDS-compliance, for example) and "face-stripping", i.e., defacing steps that
   erase the facial parts of the MRI images. But if this is only done after the
   initial conversion to an analysis dataset, version control features lead to
   potential privacy breaches: If unanonymized data is once part of the resulting
   BIDS dataset, it stays in its history (unless someone aggressively rewrites
   the history -- which is discouraged as it threatens provenance).
   If the conversion takes care of anonymizing data right away,
   though, only anonymized files are saved in the BIDS-compliant analysis dataset.
   This way, the advantages of provenance and version control can be achieved
   without sacrificing participants privacy. Do note though that this is only
   true if the raw DICOM data is **not** made available -- this should never
   be the case though, and would compromise anonymity also in non-version control
   setups.



Step 2: Conversion
------------------

At this point, you should have a study dataset that contains all MRI acquisitions
that are relevant to you, additional data, and appropriate specifications in
various ``studyspec.json`` files. In order to get from a study dataset to a
BIDS compliant dataset, create an empty dataset. To denote that this dataset
will be the final, BIDS-converted dataset, we call it ``BIDS`` in this example.

.. runrecord:: _examples/DL-101-151-110
   :workdir: dl-101
   :language: console

   $ datalad create BIDS

This dataset is currently completely disconnected from the study dataset. In order
to link it, install the study dataset as a subdataset of ``BIDS``:

.. runrecord:: _examples/DL-101-151-111
   :workdir: dl-101
   :language: console

   $ cd BIDS
   $ datalad clone -d . ../mystudy

Finally, inside of ``BIDS``, call the :command:`hirni-spec2bids` command.
If called without any arguments, the command will consult all specification
files in the study dataset and convert everything according to the specification
snippets it finds. If it is called with the ``--anonymize`` option, it will
perform the conversion anonymized. By default, this means that the ``anon-subject``
IDs are used in file names, and that all paths that are recorded during the
conversion are encrypted.

.. todo::

   what is a sidecar? Find out, add to run chapter. https://en.wikipedia.org/wiki/Sidecar_file

Lastly, the ``--only-type`` flag allows to limit the conversion to executing
only those snippets that match the given type. This allows you to convert only
subsets of your study dataset, or check whether a single or small set of
conversions works as you intend it to work without converting all of the dataset.

.. todo::

   - talk a bit about heudiconv and the software it uses?
   - how can we get images defaces? or MRIQC to run? is it part of the defaults?
   - do a conversion in a runrecord


.. rubric:: Footnotes

.. [#f1] To re-read about DataLad's run-procedures, check out section :ref:`procedures`.

.. [#f2] A ``dataset_description.json`` file exists because this file is
         `required <https://bids-specification.readthedocs.io/en/derivatives/03-modality-agnostic-files.html#dataset_descriptionjson>`_
         for valid BIDS datasets. Even if you are just in the planning phase of your
         study, you will still be able to already populate the template with you study's
         information.

.. [#f3] To re-read on capturing software environments as containers in datasets,
         go back to section :ref:`containersrun`.

.. [#f4] The web tool relies on ``datalad-webapp``. It is another DataLad extension
         that is automatically installed
         as a dependency of ``datalad-hirni``. Please note: Should you install
         ``datalad-hirni`` in its development version directly from within its Git
         repository, relevant resources for the webapp need to be retrieved by hand.
         To do this, run ``git annex get`` in ``hirni``\s Git repository.

.. [#f5] The chapter

         .. todo::

            Write metalad chapter

         introduces DataLad's metadata capabilities and demonstrates the metadata
         aggregation process in detail.

.. [#f6] Re-read about configurations and how to set them in the chapter
         :ref:`chapter_config`.