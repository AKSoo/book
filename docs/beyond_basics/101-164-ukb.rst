.. _ukbintro:

Being FAIR at scale
-------------------

More and more research data is made publicly available for analysis and reuse,
but data and data analyses need to become `FAIR <https://www.go-fair.org/fair-principles/>`_:
Findable, Accessible, Interoperable, and Reusable.
This requires comprehensive, machine- and human-readable descriptions of all
components and analyses steps, and DataLad provides many commands and tools
that can enable FAIR data sharing and processing.

Nevertheless, being FAIR on a small scale can already be a complex undertaking [#f1]_,
and being FAIR at a large scale involves a range of additional difficulties.
Here, however, we describe a data analysis usecase of a dataset that exceeds
even the limits of exa-scale computing infrastructure:
FAIR preprocessing of the `UKBiobank dataset <https://www.ukbiobank.ac.uk/>`_, a
large and continuously growing collection of data that's size demands easily
grow beyond what even supercomputing centres can provide.

This usecase, for one, shows how DataLad-centric workflows scale to gigantic
datasets and to supercomputing infrastructure. But beyond this, this chapter
demonstrates how FAIR processing is possible despite limitations on disk
space and inodes that would normally make merely *storing the raw data* impossible.


Background
^^^^^^^^^^

FAIR preprocessing of the UKBiobank data was undertaken in a joint effort from
several institutes within the
`Institute of Neuroscience and Medicine <https://www.fz-juelich.de/inm/EN/Home/home_node.html>`_
at the `Juelich Research centre <https://www.fz-juelich.de/portal/EN/Home/home_node.html>`_,
and performed on computing infrastructure of the
`INM-7 <https://www.fz-juelich.de/inm/inm-7/EN/Home/home_node.html>`_
and the `Juelich Supercomputing Centre (JSC) <https://www.fz-juelich.de/ias/jsc/EN/Home/home_node.html>`_.
This first section will introduce the project in general, and outline some
of the complexities that it involved.
The upcoming sections detail how individual difficulties were addressed.

The UKBiobank dataset
"""""""""""""""""""""

The UKBiobank project is a large long-term study that is collecting longitudinal
data on about 500.000 volunteers for a duration of at least 30 years, starting in 2006.
It aims to study the influence of genetic predispositions and environmental factors
on the development of diseases, and collects and distributes a variety of health
and life style related data.
Among other data, the UKBiobank dataset contains genetic data from blood samples,
data on cognitive function, mental health, diet, and physical activity of participants,
and some participant data is also linked to health records from general
practitioners and hospitals.
In 2014, a subset of the sample was recruited for collecting additional imaging
data as part of the `MRI Imaging study <https://www.ukbiobank.ac.uk/scanning-study-launches/>`_.
In total, 100.000 participant's brains will be scanned in the wake of this addition,
making the imaging study component the largest brain imaging study ever conducted.
By now, data from several ten thousand participants is already released.

The vast amount of available data makes the UKBiobank data an invaluable and
unparalleled research resource. It allows to study an incredibly broad range of
questions across a variety of fields that are relevant for public health.
The neuroimaging component is of utmost importance to especially neuroscientists.
In conjunction with all other available data it allows research on complex
questions on a gigantic scale.

.. findoutmore:: What brain imaging data is available?

    For imaging data, six modalities of data are available: T1-weighted
    structural images, T2-FLAIR structural images, resting-state fMRI (rfMRI),
    diffusion-weighted images (dMRI), task fMRI with a task that engages a range
    of high-level cognitive systems, and susceptibility weighted imaging (SWI) data.
    The data is released in DICOM and NifTi format, with the latter being the
    recommended option as it involves downloading far fewer files, and includes
    output from the processing pipeline such as network
    matrices for resting state data. More information can be found in the
    `UKB's brain imaging documentation <http://biobank.ctsu.ox.ac.uk/crystal/crystal/docs/brain_mri.pdf>`_.


Plan of action & difficulties for data analysis
"""""""""""""""""""""""""""""""""""""""""""""""

The UKBiobank dataset is too large for standard scientific clusters. At the time
this project was carried out, data of 40.000 subjects was released. The
compressed, downloaded raw data in NifTI format was expected to take up about
72TB of space, and standard preprocessing procedures performed on this data
would add derivatives of roughly 3 times the size of the original data. When
projecting these size demands linearly to the complete set of 100.000 participants,
the total space required for only the derivatives (no actual analyses yet)
would be roughly 0.5PB.
Therefore, retrieving and preprocessing the data required the use of the
supercomputing infrastructure at the Juelich Research Centre. The JSC granted
800TB of space on its data access server
`JUDAC <https://www.fz-juelich.de/ias/jsc/EN/Expertise/Datamanagement/JUDAC/JUDAC_node.html>`_
(+300TB in archive space) for a storage project, and sufficient computing hours
on its exascale cluster
`JURECA <https://www.fz-juelich.de/ias/jsc/EN/Expertise/Supercomputers/JURECA/JURECA_node.html>`_
in an associated compute project.

Despite access to a supercomputer with large amounts of available diskspace,
a large number of difficulties needed to be navigated throughout this project.
In order to be transparent and FAIR, DataLad-centric workflows had to be created,
adjusted to supercomputing infrastructure (such as the batch scheduling system
`SLURM <https://slurm.schedmd.com/documentation.html>`_), and thoroughly tested
with regard to scalability. In order to retrieve and store raw data, each participant's data
needed to be downloaded into its own DataLad dataset using the ``datalad-ukbiobank``
extension (introduced in the next section). Because preprocessing was to be performed
using `fMRIprep <https://fmriprep.readthedocs.io/en/stable/>`_, data had to be
`BIDS <https://bids.neuroimaging.io/>`_-compliant. Storage of the datasets required
a :term:`remote indexed archive (RIA) store` [#f2]_ of yet unseen dimensions, and easy
access to the store requires a UKBiobank superdataset with all participants. Not
only was such a large amount of subdatasets never attempted before, but when the
imaging component of the study is completed and released, 100.000 participant
subdatasets would also surpass the 64k subdirectory limit of common file systems.

Storage-wise, even with a super-sized storage project allocated to this
effort, workflows needed to be adjusted to cope with insufficient file storage
space: While the allocated disk space was adequate for storing raw and preprocessed
data, the storage project only permitted a maximum of 4.4 million inodes.

.. findoutmore:: What are inodes?

   In Unix file systems, *inodes* are file-system objects such as files or
   directories. One can create as many directories and files as the system
   has available inodes. Once a device runs out of inodes, no new files or
   directories can be created, even if there is free disk space.

Given the initial 40k participants to be preprocessed, this allowed 110
files/directories per participant.
The selected raw data of each participant alone, however, yielded about 1000 files,
and the targeted preprocessing with `fMRIprep <https://fmriprep.readthedocs.io/en/stable/>`_
and `freesurfer <https://surfer.nmr.mgh.harvard.edu/>`_ would result in some
additional 450 output files. During preprocessing, moreover, fMRIprep produces
working directories that inflate to about 15 to 20GB per subject, and some
15.000 temporary files.

Beyond storage and computing, file transfers were another issue.
In order to parallelize data downloads from the UKBiobank, downloading processes
were scheduled using
`HTCondor <https://research.cs.wisc.edu/htcondor/>`_ (a job scheduler)
and `DAGman <https://research.cs.wisc.edu/htcondor/dagman/dagman.html>`_ (a
meta-scheduler for HTCondor) on a conventional-scale scientific compute
cluster (the cluster ``juseless`` [#f3]_ of the
`INM-7 <https://www.fz-juelich.de/inm/inm-7/EN/UeberUns/ueberUns_node.html>`__)
and transferred to the Data Access server JUDAC via :term:`SSH`. To not exhaust
inodes with the raw data alone, the data needed to be exported into the RIA
stores on JUDAC as archives. During the computations, also in order to
not exhaust inodes, UKBiobank data needed to be temporarily staged on
``scratch/`` directories of the supercomputer, and transferred back into dedicated
RIA stores for derived outputs.


.. todo::

   outline a sketch of employed procedures:

   - UKBiobank data is retrieved and version controlled with the datalad ukbiobank extension,
     and assembled to a complete superdataset.

     - Challenge: BIDS conformity needs to be assured
     - Challenge: simulations are required to test whether tools scale
     - Challenge: Disk usage and inode usage limitation (this makes DataLad actually required!)
     - Challenge: Store/Dataset layout that is within file system subdirectory limits

   - data transfer to a supercomputer for computing

     - Challenge: RIA store layout, data transfer times?
     - ria-export

   - FAIR processing: using software containers and singularity to preprocess the
     data with full provenance capture

     - Challenge: software installation on a supercomputer

   - fMRIprep

     - Challenge: BIDS conformity
     - Challenge: inode consumption during computing

   - CAT preprocessing?
   - output storage

     - Challenge: setting up scalable, robust storage solution to assemble the outputs

The upcoming sections detail how each of these challenges were addressed.
Eventually, however, this project achieved a completely provenance-tracked and
FAIR preprocessing workflow on exa-scale.


.. rubric:: Footnotes

.. [#f1] Checkout chapter :ref:`chapter_yoda` for the basics of FAIR data analyses.

.. [#f2] For more on RIA stores, check out the usecase :ref:`usecase_datastore`.

.. [#f3] Why is the cluster called "``juseless``"? Because a democratic grass-root movement
         from the INM-7 wanted to take the piss out of the research centre's
         marketing compulsion to prefix any single thing they produce with "JU".