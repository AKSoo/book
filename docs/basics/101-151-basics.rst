.. _hirnibasics:

The Basic Hirni Workflow
------------------------

A ``hirni`` workflow comprises of two steps:

1. The generation of a *study dataset* with raw acquisition data and conversion-relevant
   metadata
2. The conversion of the raw data into a BIDS-compliant dataset

Ideally, the first step is an ongoing routine during the data acquisition phase of
the study: With each new participant scanned and tested, their data is imported
into a continuously growing study dataset. This section demonstrates this
continuous type of ``hirni`` workflow. But it is also possible to start
the first step only after all data is collected, and the next section
demonstrates this on a concrete example.
Once step one is accomplished, the second step is a single command, and the
result is a BIDS-compliant dataset with complete provenance reaching all the
way back to the raw data from the MR scanner.

During the conversion step, ``hirni`` relies completely on conversion-relevant
metadata and *specifications* for the conversion:
Which DICOM files constitute a NifTi file? Which modality is it?
What type of task was used? Shall the subject identifier be anonymized, or
shall a T1 weighted image file be defaced? Metadata and specifications are generated from
DICOM metadata or specified with arguments, but they can also be manually created,
extended, or edited. This semi-automatic process allows for the necessary
flexibility when converting raw data from complex acquisitions. It allows to
exclude failed acquisitions, fix errors that occurred during meta data curation
during scanning, specify whether anonymized or unanonimyzed subject identifiers
shall be used, and add steps such as custom defacing procedures to the conversion.

To understand the basic workflow, it can be helpful to have some basic
understanding of how imaging data looks like when it leaves an MRI scanner.
If you want to read up on this, take a look at the hidden section below.

.. findoutmore:: On some basics of DICOMs

   An MRI scanner will return the data it collected as DICOM (Digital Imaging
   and Communications in Medicine) files. DICOM  is an international
   standard to transmit, store, process, and display medical imaging information.
   It incorporates standards for many imaging modalities, among them magnetic
   resonance imaging (MRI), and includes protocols for image exchange, compression,
   and visualization.

   DICOM differs from other image formats in that it groups information into data sets,
   i.e., collections of information. A DICOM file consists of a *header* and
   *image data sets*. The information within the header is organized
   as a constant and standardized series of *tags* (two-number codes) that describe
   properties of the data. By extracting data from these tags one can access important
   information regarding the subject demographics, device, imaging sequence, and
   image specifics (e.g., dimensions of the image, slice thickness, or slice order).
   Apart from the actual image data, DICOM files therefore encompass also vast
   amounts of metadata.

   This structure and metadata, however, makes DICOM files also large and complex.
   Therefore, imaging data is transformed into simpler image formats that retain only
   a limited, relevant set of the images' metadata. The most common and widely
   adapted format for this is NifTi.
   It consists of fixed length header data with metadata about the
   image, and the image data. One NifTi file is an aggregation of several DICOM
   files, and its header data needs to be extracted or inferred from the correct
   DICOM tags. The NifTi header metadata contains at least the image matrix
   dimensions, the spatial resolution, the pixel depth, and the photometric
   interpretation, and enables software applications to recognize and correctly
   open the associated image.

   DICOMs are usually grouped into *acquisition*-wise tarballs. This means that the
   scanner produces and deposits one ``.tar.gz`` archive with all DICOMs that
   were acquired together into a scanner database, such as an
   `XNAT server <https://www.xnat.org/about/>`_. Depending on the length and type
   of data acquisition, such a tarball can contain thousands of DICOM files for
   a single subject. One acquisition usually consist of
   data from several *MRI sequences*: In a single acquisition, a typical MRI
   study collects anatomical images, and study-specific data such as task-based and/or
   resting state fMRI data,
   `diffusion-weighted MRI data <https://en.wikipedia.org/wiki/Diffusion_MRI>`_,
   `MR angiography <https://en.wikipedia.org/wiki/Magnetic_resonance_angiography>`_,
   or other `sequences <https://en.wikipedia.org/wiki/MRI_sequence>`_.
   All DICOM files that belong to the same sequence are called
   an *image series*. Thus, an acquisition tarball contains several image series
   that constitute data from different sequences, acquired in one session for a
   single subject. Usually, the DICOM identifier in the DICOM header contains
   information on which files belong to the same series and in which order.
   During conversion, each image series needs to be identified,
   sorted, and stacked into a NifTI file for the given sequence type.

Step 1: Creation of a study dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once ``hirni`` is installed, we can create a standard dataset.
In the beginning, it is a regular DataLad dataset:

.. runrecord:: _examples/DL-101-151-101
   :language: console
   :workdir: dl-101

   $ datalad create mystudy

By installing ``hirni``, DataLad got access to a new :term:`run procedure` [#f1]_:
``cfg_hirni``. This procedure can create a study dataset skeleton inside of
``mystudy``, thus preparing ``mystudy`` to become the *study dataset*.

.. runrecord:: _examples/DL-101-151-102
   :language: console
   :workdir: dl-101

   $ cd mystudy
   $ datalad run-procedure cfg_hirni

Here is the layout of the fresh study dataset:

.. runrecord:: _examples/DL-101-151-103
   :language: console
   :workdir: dl-101/mystudy

   $ tree -L 3

The study dataset skeleton contains a ``README`` file and two ``.json`` files:

- ``studyspec.json``, which will contain some of the relevant specifications for
  conversion with ``hirni``, and
- `dataset_description.json <https://bids-specification.readthedocs.io/en/derivatives/03-modality-agnostic-files.html#dataset_descriptionjson>`_,
  a BIDS-compliant template file with fields to describe the dataset [#f2]_


Furthermore, ``cfg_hirni`` created a subdirectory ``code/`` and installed the
``hirni-toolbox`` inside of it.

.. findoutmore:: What's inside the toolbox?

   The ``cfg_hirni`` procedure populated the study dataset with a ``code/``
   directory. Inside, it installed a subdataset, ``hirni-toolbox``:

   .. runrecord:: _examples/DL-101-151-104
      :language: console
      :workdir: dl-101/mystudy

      $ datalad subdatasets

   The toolbox is ``hirni``\s collection of tools for the tasks you may want it
   to perform for you. For this, it references potentially relevant
   code and software for neuroimaging data.

   A collection of procedures in ``procedures/`` references
   scripts for operations such as conversion with `heudiconv <https://github.com/nipy/heudiconv>`_,
   defacing with `mridefacer <https://johnmuschelli.com/fslr/reference/mridefacer.html>`_,
   or quality control with `MRIQC <https://mriqc.readthedocs.io/en/stable/>`_.
   The potentially required software for these procedures (such as
   `FSL <https://fsl.fmrib.ox.ac.uk/fsl/fslwiki>`_, `heudiconv <https://github.com/nipy/heudiconv>`_,
   ...) is distributed alongside the procedures as Singularity images that can
   be retrieved automatically if the procedures that require them are used [#f3]_.

   You can find the toolbox dataset on GitHub at
   `github.com/psychoinformatics-de/hirni-toolbox <https://github.com/psychoinformatics-de/hirni-toolbox>`_
   if you want to take a closer look.

Initial dataset information
"""""""""""""""""""""""""""

Let's say you have just started to prepare your neuroimaging study.
Even if you havn't acquired a single DICOM file yet, you can already curate
general metadata about your study into the study dataset. This helps to capture
information that you now know from the top of your head, but that you would
later need to dig through old documents for to find out about.
You can, however, also get to this step at any later point in time, or tackle it
if you are already sitting on a pile of acquired DICOM data, as will be demonstrated
in the upcoming section.

It is recommended to describe your study for humans in the ``README`` file, and fill
out missing values in ``dataset_description.json``. For the ``README``, you can
get creative. There is no format you need to adhere to. If you want, consider
using the handbooks DataLad dataset template found at :ref:`dataset_textblock`.
The contents of ``dataset_description.json`` currently are an empty template (note that
we pipe the output of the ``cat`` command to `jq <https://stedolan.github.io/jq/>`_,
a command line JSON processor that prettifies the output):

.. runrecord:: _examples/DL-101-151-105
   :language: console
   :workdir: dl-101/mystudy

   $ cat dataset_description.json | jq

Most of the fields will be straightforward to answer for you, and you should input
values for them into the empty strings or JSON lists the template provides.
You can either do this with an editor of your choice and save your changes with
:command:`datalad save` afterwards, or use ``hirni``\s webUI for assistance. If
you do it by hand and are not too familiar with JSON, be mindful to not create
invalid JSON by accidentally removing commas or forgetting about quotes!
There are helpful commandline
tools such as `jq <https://stedolan.github.io/jq/>`_ or webtools such as
`JSONlint <https://jsonlint.com/>`_ to help verify whether your files stayed valid.

.. findoutmore:: Using the webUI

   Let's take a quick look into how web assisted curation works. In order to fill
   out ``dataset_description.json``, start the web tool [#f4]_ with

   .. code-block:: bash

      $ datalad webapp --dataset . hirni

   You will be able to open ``http://127.0.0.1:5000/`` in a browser and see the
   following interface:

   .. figure:: ../artwork/src/hirni_sc1.png

   By clicking "Edit Study Metadata" you will be directed to a form with the
   same fields as ``dataset_description.json``. The text fields contain short
   help or example messages that you can replace. There is no need to fill in
   all fields at once -- leave fields with information that you don't yet have blank.
   You can return to this file later, and also edit existing contents.
   As soon as you are done filling out as much as you can or like, click the
   ``Save to dataset`` button at the end of the form.

   .. figure:: ../artwork/src/hirni_sc2.png

   Internally, this process writes your input to the ``dataset_description.json``
   file and concludes with a ``datalad save``, using the commit message
   ``"[HIRNI] Add specification snippet for: dataset_description.json"``

   There are two advantages to using the webUI: For one, it will write valid JSON.
   And secondly, as the webUI is only ever used by humans, data that is curated
   via this web tool has a label of confidence attached to it. This is not yet
   relevant for ``dataset_description.json``, but for editing ``studyspec.json``
   files later. In their JSON dictionaries, an ``"approved": "true"`` key will
   indicate if any information was given by a human and alter the webUI's
   handling of this information piece afterwards.

   Using the webUI for acquisition metadata will have a slightly more complex
   layout, but a separate findoutmore further down in this section will shed
   light on it.

Importing DICOMs
""""""""""""""""

As soon as the first acquisition exists, DICOM data can be imported into the study
dataset. A DICOM acquisition import takes the following structure in your study dataset:

.. code-block:: bash
   :emphasize-lines: 1-7

   ├── acquisition1             # subdirectory
   │   └── dicoms               # subdataset
   │       └──  ...
   │   ├── studyspec.json
   │   ├── protocol.pdf
   │   └── physio
   │       └──  ...
   ├── code
   │   └── hirni-toolbox
   │       ├── analyses
   │       ├── converters
   │       ├── data-retrieval
   │       ├── postprocessing
   │       └── procedures
   ├── dataset_description.json
   ├── README
   └── studyspec.json

Let's decipher that:

- There will be one subdirectory per acquisition tarball. In the above example,
  there is one acquisition, and it is represented in the subdirectory ``acquisition1``.
- Inside of this subdirectory, there will be at minimum a ``studyspec.json`` file
  with the specifications of this acquisition, and a ``dicoms/`` subdataset that
  contains the extracted DICOMs of the acquisition.
- Optionally, other study relevant data such as the
  acquisition protocol or physiological data can be added.

.. index:: ! datalad command; hirni-import-dcm

This structure is automatically created if ``hirni`` is pointed to the DICOM
tarball with the :command:`datalad hirni-import-dcm`
command. This command needs a path or url to a DICOM tarball of one acquisition,
and an acquisition ID that will become the subdirectory name. In the file hierarchy
above, this acquisition ID was ``acquisition1``.
Additionally, the command can take a few specifications that will be relevant for
conversion as optional command line arguments: You can specify the subject identifier as used
during scanning with the ``--subject`` option, and an anonymized ID (such as ``001``)
that this subject should be identified with in the final BIDS dataset
with the ``--anon-subject`` option.
Note that *some* of this information can be inferred from DICOM headers if
it was correctly filled in during scanning, for example the acquisition ID and
the subject identifier.

To import the first acquisition of the first subject, you run a command similar
to this one inside of your study dataset::

   $ datalad hirni-import-dcm --anon-subject 001 path/to/DICOMs.tar.gz acquisition1

Throughout your data acquisition, or -- if everything is already acquired -- in
one go afterwards, you import all acquisitions of the study into your study
dataset.

.. todo::

   ``--properties`` option

Semi-automatic metadata extraction and conversion specification
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

Ultimately, the :command:`datalad hirni-import-dcm` serves only one goal:
Collect all relevant information from the DICOM headers to enable the conversion.
This information is stored as aggregated metadata from the subdataset
in ``.datalad/metadata`` [#f5]_, and as acquisition-wise specifications in
``<acquisition-ID>/studyspec.json``.
You can find out more about the details of this process below.

.. findoutmore:: Internals of this command

   :command:`datalad hirni-import-dcm` executes a number of commands in order to
   prepare the conversion of the study dataset.

   Ultimately, DICOMs are only extracted from the tarball archive to access and
   query their headers' metadata. For some of this metadata extraction, the ``dicom``
   :term:`extractor` of ``datalad-neuroimaging`` is used. Together with
   ``datalad-metalad``, this extractor can record metadata from standard DICOM files,
   and, among other things, group together DICOM files that belong to an image series,
   i.e., a single sequence within the acquisition.
   The resulting metadata is *aggregated* into the study dataset [#f5]_.

   Beyond that, ``hirni``\s own metadata command :command:`hirni-dicom2spec` is
   executed and prepopulated an acquisition-specific ``studyspec.json`` file
   inside of the acquisition subdirectory. This file contains specifications
   for later structuring and naming
   of converted data to BIDS-compliant structures, such as the task name.
   Unlike the metadata mentioned above, metadata in ``studyspec.json`` files can and
   should be manually edited to insert custom or yet missing conversion-relevant
   information later. During DICOM import this file is prepopulated with relevant
   fields, and prefills fields with available metadata where available,
   for example about the location and version of the files and any BIDS-relevant
   information that is available from the DICOM headers.

   All of this metadata extraction is therefore crucial for the upcoming conversion.
   and the study dataset will receive a large chunk of the DICOM metadata
   required for the conversion directly at the time of the DICOM import.

   The basis of the import and data handling is a :command:`datalad add-archive-content`
   (:manpage:`datalad-add-archive-content` manual) command. This command -- together
   with the way the data import is performed -- helps to save disk space in the
   study dataset: The ``tar.gz`` archive is saved in its compressed form into a
   :term:`branch` of the subdataset, and is extracted into the ``master`` branch
   of the dataset. From the DICOM files in ``master``, metadata can be extracted.
   With :command:`datalad add-archive-content`, the archive in the dataset branch
   is treated like a :term:`special remote`: On demand, individual files from
   ``master`` could be re-retrieved from the tarball. Therefore, the archive in
   the seperate branch is essentially a compressed DICOM storage.
   Because this ensures one available source for the data, the (larger) file
   content of the extracted DICOMs in ``master`` can be dropped
   right after metadata aggregation. Thus, instead of large, extracted DICOMs,
   the subdatasets only contains the compressed tarball, while ensuring that
   DICOM files can nevertheless be reobtained from the archive on demand.

However, not all relevant information can be inferred from the DICOM headers.
Sometimes, DICOM headers contain false information, too, or acquisitions contain
image series from failed MRI sequences that should not be converted. And while
``hirni``\s conversion *can* incorporate special procedures such as anonymizing
subject identifiers or defacing anatomical images, it still needs a human decision
to do so. For these and similar cases that require human input, manual
specifications are necessary.

.. findoutmore:: Why anonymizing measures should be done during the conversion

   Basic ethical practices in science and the General Data Protection Regulation
   (GDPR) of the European Union require that potentially identifying information
   of subjects' data needs to be anonymized to protect the identity and privacy
   rights of research participants.

   MRI data contains a lot of identifiable data. DICOM headers can contain names
   or birth dates of participants, and even subject identifiers can allow
   guestimates about the participants.
   Actual raw DICOM data from neuroimaging studies also contains faces -- a
   *very* identifying piece of information about a person. None of this information
   is therefore allowed be distributed to people that did not conduct the original
   study: In order to comply to ethical and privacy requirements, datasets with
   neuroimaging data need to be anonymized. Usually, this requires anonymous
   subject identifiers (ascending zero-padded integers for simultaneous
   BIDS-compliance, for example) and "face-stripping", i.e., defacing steps that
   erase the facial parts of the MRI images. But if this is only done after the
   initial conversion to an analysis dataset, version control features lead to
   potential privacy breaches: If unanonymized data is once part of the resulting
   BIDS dataset, it stays in its history (unless someone aggressively rewrites
   the history -- which is discouraged as it threatens provenance).
   If the conversion takes care of anonymizing data right away,
   though, only anonymized files are saved in the BIDS-compliant analysis dataset.
   This way, the advantages of provenance and version control can be achieved
   without sacrificing participants privacy. Do note though that this is only
   true if the raw DICOM data is **not** made available -- this should never
   be the case though, and would compromise anonymity also in non-version control
   setups.

During DICOM import, the command ``hirni-dicom2spec`` generated so called
*specification snippets* from DICOM metadata and stored them in the
``studyspec.json`` file.

Specifications are fundamental to ``hirni`` and indispensable for a successful
conversion. But they are also quite hard to get a good grasp on. This
findoutmore will shed some light on them.

.. findoutmore:: On specifications

   Specifications describe *data entities* of the study dataset. An entity is
   not necessarily a single file or directory, but any study dataset content
   that is one logical unit that should be processed in the same way, such as a
   DICOM series (which likely is a subset of the data in ``dicoms/``), all
   respiratory traces inside of a ``physio/`` subdirectory, or a single
   ``events.txt`` stimulation log file for the acquisition.

   For each data entity, a specification snippet defines how it should be
   converted (in other words: treated). Note that "conversion" does not exclusively
   refer to a DICOM to NifTi conversion - in ``hirni``\s terms, a conversion is
   any action that is undertaken with a file in the study dataset to get it into
   the form it should take in the BIDS dataset. This could be a simple action
   such as copying or renaming a file, but also complex, multistepped processes,
   for example defacing.

   Reading, understanding, and creating specification snippets requires some
   pre-existing knowledge on certain topics, and an understanding of the data
   and its properties.
   Specifications are defined in ``studyspec.json`` files. These files contain
   specification snippets as one line of JSON stream.

   .. findoutmore:: Some fundamentals of JSON and JSON stream

      JSON (JavaScript Object Notation) is easy to read when it is formatted
      nicely (for example with `jq <https://stedolan.github.io/jq/>`_): In JSON,
      data is stored in ``key-value`` pairs (such as ``{"name":"Joanna"}``), and
      separated by commas. Curly braces (``{}``)
      hold *objects* (dictionaries), such as

      .. code-block:: JSON

         {
          "person": {
             "name": "Joanna",
             "role": "data scientist"
             }
          }

      and square brackets (``[]``) hold *arrays* (lists), as in

      .. code-block:: JSON

         {
          "employees":[
           {
            "name": "Joanna",
            "role": "data scientist"
            },
            {
             "name": "Anna",
             "role": "software developer"
            },
            {
              "name": "Peter",
              "role": "marketing analyst"
            }
           ]
         }

      JSON has the inconvenient side effect that a single mistake in the document
      (e.g., a missing comma, or misplaced bracket) invalidates the whole document.
      Therefore, specifications are written in JSON streaming format.
      In JSON stream, the JSON essentially contains fewer linebreaks. Each
      line contains one valid JSON dictionary. This format allows per-line processing,
      and guarantees that all valid lines can be processed, even if the document
      contains some invalid lines. Here is an example of one specification snippet
      (this one belongs to the DICOMs of an acquisition). It is represented as
      a nested JSON dictionary and is one long line of text::

         $ {"anon-subject":{"approved":false,"value":"001"},"bids-acquisition":{"approved":false,"value":null},"bids-contrast-enhancement":{"approved":false,"value":null},"bids-direction":{"approved":false,"value":null},"bids-echo":{"approved":false,"value":null},"bids-modality":{"approved":false,"value":"bold"},"bids-reconstruction-algorithm":{"approved":false,"value":null},"bids-run":{"approved":false,"value":"01"},"bids-session":{"approved":false,"value":null},"bids-task":{"approved":false,"value":"oneback"},"comment":{"approved":false,"value":""},"dataset-id":"2f2a44d8-7271-11ea-861d-f9fd9dd57046","dataset-refcommit":"64644f46a471424df817dd5001f72b223c6f7a33","description":{"approved":false,"value":"func_task-oneback_run-1"},"id":{"approved":false,"value":401},"location":"dicoms","procedures":[{"on-anonymize":{"approved":false,"value":false},"procedure-call":{"approved":false,"value":null},"procedure-name":{"approved":false,"value":"hirni-dicom-converter"}}],"subject":{"approved":false,"value":"02"},"type":"dicomseries:all"}

      When processed, conversions are executed in the order of the specification
      snippets.

   A specification snippet defines a list of :term:`run procedure`\s and how
   exactly they are called. If you don't know what a procedure is, yet, please
   give section :ref:`procedures` a quick read to get an initial overview.
   Additionally, it contains fields to define potentially
   relevant BIDS terms about the data. If you are not familiar with the general
   naming scheme in BIDS, please head over to `bids.neuroimaging.io <https://bids.neuroimaging.io/>`_
   and take a look at the basic logic behind file names.
   Below, you can see the a single specification
   snippet, formatted into JSON for better readability. It is a snippet that would
   be generated automatically during DICOM import for one DICOM series:

   .. findoutmore:: see the formatted snippet

      .. code-block:: JSON

         {
          "anon-subject": {
            "approved": false,
            "value": "001"
          },
          "bids-acquisition": {
            "approved": false,
            "value": null
          },
          "bids-contrast-enhancement": {
            "approved": false,
            "value": null
          },
          "bids-direction": {
            "approved": false,
            "value": null
          },
          "bids-echo": {
            "approved": false,
            "value": null
          },
          "bids-modality": {
            "approved": false,
            "value": "bold"
          },
          "bids-reconstruction-algorithm": {
            "approved": false,
            "value": null
          },
          "bids-run": {
            "approved": false,
            "value": "01"
          },
          "bids-session": {
            "approved": false,
            "value": null
          },
          "bids-task": {
            "approved": false,
            "value": "oneback"
          },
          "comment": {
            "approved": false,
            "value": ""
          },
          "dataset-id": "2f2a44d8-7271-11ea-861d-f9fd9dd57046",
          "dataset-refcommit": "64644f46a471424df817dd5001f72b223c6f7a33",
          "description": {
            "approved": false,
            "value": "func_task-oneback_run-1"
          },
          "id": {
            "approved": false,
            "value": 401
          },
          "location": "dicoms",
          "procedures": [
            {
              "on-anonymize": {
                "approved": false,
                "value": false
              },
              "procedure-call": {
                "approved": false,
                "value": null
              },
              "procedure-name": {
                "approved": false,
                "value": "hirni-dicom-converter"
              }
            }
          ],
          "subject": {
            "approved": false,
            "value": "02"
          },
          "type": "dicomseries:all"
          "uid": "1.2.0260.1.3680043.2.1143.53213523436472938475829384762332",
        }

   .. todo::

      NOTE: UUID IS NOT CLEAR YET!

   This snipppet consists of a number of nested dictionaries. A few are potentially
   relevant BIDS terms, such as ``bids-acquisition``, or ``bids-run``. Not all of
   them will apply to your study, but they are in the template to allow their use
   should they be relevant. A few of them are prefilled with values that were
   derived from DICOM headers, such as

   .. code-block::

       [...]
       "bids-modality": {
         "approved": false,
         "value": "bold"
       },
       "bids-run": {
         "approved": false,
         "value": "01"
       },
       "bids-task": {
         "approved": false,
         "value": "oneback"
       },
       [...]

   Another dictionary is the ``anon-subject`` object. It contains the value
   provided with the :command:`hirni-import-dcm` call in the ``--anon-subject``
   flag. There also is a ``subject`` dictionary. Its value was either specified
   in :command:`hirni-import-dcm` or derived from DICOM headers. The information
   in these dictionaries will be used to create a *virtual* BIDS term ``bids-subject``
   depending on whether subject names should be anonymized. If the
   ``--anonymize`` flag is set during conversion, the ``anon-subject`` ID will
   be used in file names instead of the ``subject`` ID.

   .. code-block::

      "anon-subject": {
        "approved": false,
        "value": "001"
      },
     [...]
      "subject": {
        "approved": false,
        "value": "02"
      },


   Three name-value pairs define the dataset ID of the data entity the
   snippet refers to, a path to the entity relative to the location of the
   specfile, and the latest commit (the ``refcommit``).
   This serves to capture the provenance of the input data and its version:

   .. code-block::

      "dataset-id": "2f2a44d8-7271-11ea-861d-f9fd9dd57046",
      "dataset-refcommit": "64644f46a471424df817dd5001f72b223c6f7a33",
      [...]
      "location": "dicoms",

   A ``type`` key contains ...

   .. todo::

      NOTES FOLLOW
      - per snippet.
      - special meaning for DICOMs,
      - for all other data it is an arbitrary field that allows to group snippets in a specification file.
      - can be used category to match entries of entire specification against,
      - conversion execution can be limited to specification snippets of a particular type. (one can try out a conversion like this).
      - default value: "generic file"

      uid for dicoms is the series identifier NOT CLEAR YET - VIDEO IS INCONSISTENT.
      - The conversion with heudiconv acts on a complete dicom subdataset, i.e.
      potentially on several image series at once (HOW DOES THIS CHANGE WITH NEW
      FEATURES?)

   .. code-block::

      "type": "dicomseries:all"
      "uid": "1.2.0260.1.3680043.2.1143.53213523436472938475829384762332",

   Finally, a list holds a number of dictionaries that define the procedure(s) that
   are to be performed on the DICOMs when this snippet is processed and how those
   procedures are to be executed:

   .. code-block::

      "procedures": [
       {
         "on-anonymize": {
           "approved": false,
           "value": false
         },
         "procedure-call": {
           "approved": false,
           "value": null
         },
         "procedure-name": {
           "approved": false,
           "value": "hirni-dicom-converter"
         }
       }
      ],

   One of the dictionaries is the procedure name (``procedure-name``). This
   name refers to a :term:`run procedure`. Most of those procedures are part of
   the hirni toolbox and exist under ``code/toolbox/procedures``. Another
   dictionary is the procedure call (``procedure-call``). This defines how the
   procedure should be called - this can be preconfigured, set to a default, or
   edited manually. In the example above, no specific call format is necessary,
   but a separate findoutmore will go into the details of the syntax for procedure
   calls and demonstrate how they work. For some data, the ``on-anonymize``
   dictionary is included as a switch that determines whether the procedure runs
   during conversion if ``--anonymize`` is specified (???? IS THIS CORRECT?).

   Each dictionary has a boolean ``approve`` field. This exists for the webUI

   .. todo::

      ... COULDNT TRY THIS YET, DIDNT WORK

MORE NOTES:
- available procedures: copy-converter (copies a file

.. findoutmore:: Formatting procedure calls

   .. todo::

      - replacements: script = procedure call, ds = dataset it is called upon,
        and all other keys in the given snippet in double curly brackets

      - procedures: Should all use datalad run or datalad containers-run

.. findoutmore:: Rules

   Rules are configurations that apply settings that influence the behavior
   of ``hirni`` or its underlying tools. They can be set for different scopes
   (applying on a system-, user-, or dataset-level), and if a configuration exists
   on two or more levels with different values, more specific scopes take
   precedence over more general scopes.

   Rules are implemented as Python classes. In order to write your own set of
   rules, you will need to create a Python file that contains them. In order to
   apply the rules, you need to add them -- just like the configurations
   introduced in chapter :ref:`chapter_config` -- with :command:`git config` on
   a scope level of your choice. The key of the configuration is
   ``datalad.hirni.dicom2spec.rule`` and its value is an absolute path to your
   Python file.

   In order to write a rule, write a Python class with the help of the
   template file that ``hirni`` provides at....

   .. todo::

      AARRRGHHH oh my god, lets stick with "Rules exist, their documentation can be
      summarized in the word TODO, have fun dying"

   Notes from April 9th call: Building custom rules

   - rules are based on extracted dicom metadata, DICOMs themselves are not needed.
   - specifications in studyspec.json are based on rules
   - The dicom2spec command applies the rules - no reimporting is necessary, just
     rerun dicom2spec with updated rules
   - datalad hirni comes with a custom rules template (in Github repository). Copy the
     template into some place. Configure datalad to use these rules whenever
     dicom2spec is used. ben does it with a local configuration:
     git config dataladhirni.dicom2spec.rules <path/to/python file>
   - check whether this worked by adding a pdb statement into the rule python file
   - how do rules work: We are specifying a class, the attribute at the end of the file
     tells hirni which the rule is (if there are several classes)
   - class needs at least 2 methods, constructor and call method. whenever an object
     of this class is generated it gets DICOM metadata for the entire aq that
     dicom2spec was called on
   - dicommetadata is a list of dictionaries with extracted DICOM header fields
   - rules are applied per image series (call method is called once per image series).
     whenever call method is called, it should return a list of dictionaries that goes into the
     specification.
   - any number of keys is possible. The special stuff about the virtual bids-subject
     key.
   - invalidation of particular image series: This can be done scanner-manufacturer-wise
   - specification dicts is the specification file (each dict in this list will be a line
     in the studyspec file)
   - guess functions in standard rules work on protocol name
   - the approved key is added automatically by hirni depending on what call method
     returns. aproved is false by default. webui can distinguish false and true
     values. all false will show the fields as editable and asks for approval, all
     trues
   - no procedure definition in dicom2spec

Adding other data
"""""""""""""""""

Neuroimaging studies usually encompass more than only imaging data. Depending
on the study, there may be measures from additional modalities (such as
physiological or behavioral data), stimulation protocols, acquisition protocols
from the scanner, or other digital data.

As it is part of each acquisition, this additional data should be added into
each acquisition subdirectory.
In order to add additional data, simply copy it into the acquisition subdirectory
(not into the ``dicoms/`` subdataset!). Afterwards, save the addition with
:command:`datalad save` and a helpful commit message.
It does not need to follow any particular
structure, but in order to be part of the conversion (i.e., added to the
BIDS-compliant dataset in the correct subject and session directories, named
correctly and BIDS-compliant, and potentially processed in a file-specific
way), it needs to get a specification.
There are commands to *help* with that, but there is no fully automated
specification derivation for additional data. Here is a sketch of how the
process looks like:

.. todo::

   Sketch how to do a specification





Conversion
""""""""""

At this point, you should have a study dataset that contains all MRI acquisitions
that are relevant to you, additional data, and appropriate specifications in
various ``studyspec.json`` files. In order to get from a study dataset to a
BIDS compliant dataset, create an empty dataset. To denote that this dataset
will be the final, BIDS-converted dataset, we call it ``BIDS`` in this example.

.. runrecord:: _examples/DL-101-151-110
   :workdir: dl-101
   :language: console

   $ datalad create BIDS

This dataset is currently completely disconnected from the study dataset. In order
to link it, install the study dataset as a subdataset of ``BIDS``:

.. runrecord:: _examples/DL-101-151-111
   :workdir: dl-101
   :language: console

   $ cd BIDS
   $ datalad clone -d . ../mystudy

Finally, inside of ``BIDS``, call the :command:`hirni-spec2bids` command.
If called without any arguments, the command will consult all specification
files in the study dataset and convert everything according to the specification
snippets it finds. If it is called with the ``--anonymize`` option, it will
perform the conversion anonymized. By default, this means that the ``anon-subject``
IDs are used in file names, and that all paths that are recorded during the
conversion are encrypted.

.. todo::

   what is a sidecar? Find out, add to run chapter. https://en.wikipedia.org/wiki/Sidecar_file

Lastly, the ``--only-type`` flag allows to limit the conversion to executing
only those snippets that match the given type. This allows you to convert only
subsets of your study dataset, or check whether a single or small set of
conversions works as you intend it to work without converting all of the dataset.

.. todo::

   - talk a bit about heudiconv and the software it uses?
   - how can we get images defaces? or MRIQC to run? is it part of the defaults?
   - do a conversion in a runrecord


.. rubric:: Footnotes

.. [#f1] To re-read about DataLad's run-procedures, check out section :ref:`procedures`.

.. [#f2] A ``dataset_description.json`` file exists because this file is
         `required <https://bids-specification.readthedocs.io/en/derivatives/03-modality-agnostic-files.html#dataset_descriptionjson>`_
         for valid BIDS datasets. Even if you are just in the planning phase of your
         study, you will still be able to already populate the template with you study's
         information.

.. [#f3] To re-read on capturing software environments as containers in datasets,
         go back to section :ref:`containersrun`.

.. [#f4] The web tool relies on ``datalad-webapp``. It is another DataLad extension
         that is automatically installed
         as a dependency of ``datalad-hirni``. Please note: Should you install
         ``datalad-hirni`` in its development version directly from within its Git
         repository, relevant resources for the webapp need to be retrieved by hand.
         To do this, run ``git annex get`` in ``hirni``\s Git repository.

.. [#f5] The chapter

         .. todo::

            Write metalad chapter

         introduces DataLad's metadata capabilities and demonstrates the metadata
         aggregation process in detail.