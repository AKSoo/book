.. _hirnibasics:

The Basic Hirni Workflow
------------------------

A ``hirni`` workflow comprises of two steps:

1. The generation of a *study dataset* with raw acquisition data and conversion-relevant
   metadata
2. The conversion of the raw data into a BIDS-compliant dataset

Ideally, the first step is an ongoing routine during the data acquisition phase of
the study: With each new participant scanned and tested, their data is imported
into a continuously growing study dataset. It is also possible to start
the first step only after all data is collected, though.
Once step one is accomplished, the second step is a single command, and the
result is a BIDS-compliant dataset with complete provenance reaching all the
way back to the raw data from the MR scanner.

During the conversion step, ``hirni`` relies completely on conversion-relevant
metadata and *specifications* for the conversion:
Which DICOM files constitute a NifTi file? Which modality is it?
What type of task was used? Shall the subject identifier be anonymized, or
shall a T1 weighted image file be defaced? Metadata and specifications are generated from
DICOM metadata or specified with arguments, but they can also be manually created,
extended, or edited. This semi-automatic process allows for the necessary
flexibility when converting raw data from complex acquisitions. It allows to
exclude failed acquisitions, fix errors that occurred during meta data curation
during scanning, specify whether anonymized or unanonimyzed subject identifiers
shall be used, and add steps such as custom defacing procedures to the conversion.

To understand the basic workflow, it can be helpful to have some basic
understanding of how imaging data looks like when it leaves an MRI scanner.
If you want to read up on this, take a look at the hidden section below.

.. findoutmore:: On some basics of DICOMs

   An MRI scanner will return the data it collected as DICOM (Digital Imaging
   and Communications in Medicine) files. DICOM  is an international
   standard to transmit, store, process, and display medical imaging information.
   It incorporates standards for many imaging modalities, among them magnetic
   resonance imaging (MRI), and includes protocols for image exchange, compression,
   and visualization.

   DICOM differs from other image formats in that it groups information into data sets,
   i.e., collections of information. A DICOM file consists of a *header* and
   *image data sets*. The information within the header is organized
   as a constant and standardized series of *tags* (two-number codes) that describe
   properties of the data. By extracting data from these tags one can access important
   information regarding the subject demographics, device, imaging sequence, and
   image specifics (e.g., dimensions of the image, slice thickness, or slice order).
   Apart from the actual image data, DICOM files therefore encompass also vast
   amounts of metadata.

   This structure and metadata, however, makes DICOM files also large and complex.
   Therefore, imaging data is transformed into simpler image formats that retain only
   a limited, relevant set of the images' metadata. The most common and widely
   adapted format for this is NifTi.
   It consists of fixed length header data with metadata about the
   image, and the image data. One NifTi file is an aggregation of several DICOM
   files, and its header data needs to be extracted or inferred from the correct
   DICOM tags. The NifTi header metadata contains at least the image matrix
   dimensions, the spatial resolution, the pixel depth, and the photometric
   interpretation, and enables software applications to recognize and correctly
   open the associated image.

   DICOMs are usually grouped into *acquisition*-wise tarballs. This means that the
   scanner produces and deposits one ``.tar.gz`` archive with all DICOMs that
   were acquired together into a scanner database, such as an
   `XNAT server <https://www.xnat.org/about/>`_. Depending on the length and type
   of data acquisition, such a tarball can contain thousands of DICOM files for
   a single subject. One acquisition usually consist of
   data from several *MRI sequences*: In a single acquisition, a typical MRI
   study collects anatomical images, and study-specific data such as task-based and/or
   resting state fMRI data,
   `diffusion-weighted MRI data <https://en.wikipedia.org/wiki/Diffusion_MRI>`_,
   `MR angiography <https://en.wikipedia.org/wiki/Magnetic_resonance_angiography>`_,
   or other `sequences <https://en.wikipedia.org/wiki/MRI_sequence>`_.
   All DICOM files that belong to the same sequence are called
   an *image series*. Thus, an acquisition tarball contains several image series
   that constitute data from different sequences, acquired in one session for a
   single subject. Usually, the DICOM identifier in the DICOM header contains
   information on which files belong to the same series and in which order.
   During conversion, each image series needs to be identified,
   sorted, and stacked into a NifTI file for the given sequence type.

Step 1: Creation of a study dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once ``hirni`` is installed, we can create a standard dataset.
In the beginning, it is a regular DataLad dataset:

.. runrecord:: _examples/DL-101-151-101
   :language: console
   :workdir: dl-101

   $ datalad create mystudy

By installing ``hirni``, DataLad got access to a new :term:`run procedure` [#f1]_:
``cfg_hirni``. This procedure can create a study dataset skeleton inside of
``mystudy``, thus preparing ``mystudy`` to become our *study dataset*.

.. runrecord:: _examples/DL-101-151-102
   :language: console
   :workdir: dl-101

   $ cd mystudy
   $ datalad run-procedure cfg_hirni

Here is the layout of the fresh study dataset:

.. runrecord:: _examples/DL-101-151-103
   :language: console
   :workdir: dl-101/mystudy

   $ tree -L 3

The study dataset skeleton contains a ``README`` file and two ``.json`` files:

- ``studyspec.json``, which will contain some of the relevant specifications for
  conversion with ``hirni``, and
- `dataset_description.json <https://bids-specification.readthedocs.io/en/derivatives/03-modality-agnostic-files.html#dataset_descriptionjson>`_,
  a BIDS-compliant template file with fields to describe the dataset [#f2]_


Furthermore, ``cfg_hirni`` created a subdirectory ``code/`` and installed the
``hirni-toolbox`` inside of it.

.. findoutmore:: What's inside the toolbox?

   The ``cfg_hirni`` procedure populated the study dataset with a ``code/``
   directory. Inside, it installed a subdataset, ``hirni-toolbox``:

   .. runrecord:: _examples/DL-101-151-104
      :language: console
      :workdir: dl-101/mystudy

      $ datalad subdatasets

   The toolbox is ``hirni``\s collection of tools for the tasks you may want it
   to perform for you. For this, it references potentially relevant
   code and software for neuroimaging data.

   A collection of procedures in ``procedures/`` references
   scripts for operations such as conversion with `heudiconv <https://github.com/nipy/heudiconv>`_,
   defacing with `mridefacer <https://johnmuschelli.com/fslr/reference/mridefacer.html>`_,
   or quality control with `MRIQC <https://mriqc.readthedocs.io/en/stable/>`_.
   The potentially required software for these procedures (such as
   `FSL <https://fsl.fmrib.ox.ac.uk/fsl/fslwiki>`_, `heudiconv <https://github.com/nipy/heudiconv>`_,
   ...) is distributed alongside the procedures as Singularity images that can
   be retrieved automatically if the procedures that require them are used [#f3]_.

   You can find the toolbox dataset on GitHub at
   `github.com/psychoinformatics-de/hirni-toolbox <https://github.com/psychoinformatics-de/hirni-toolbox>`_
   if you want to take a closer look.

Initial dataset information
"""""""""""""""""""""""""""

Let's say you have just started to prepare your neuroimaging study.
Even if you havn't acquired a single DICOM file yet, you can already curate
general metadata about your study into the study dataset. This helps to capture
information that you now know from the top of your head, but that you would
later need to dig through old documents for to find out about.
You can, however, also get to this step at any later point in time, or tackle it
if you are already sitting on a pile of acquired DICOM data.

It is recommended to describe your study for humans in the ``README`` file, and fill
out missing values in ``dataset_description.json``. For the ``README``, you can
get creative. There is no format you need to adhere to. If you want, consider
using the handbooks DataLad dataset template found at :ref:`dataset_textblock`.
The contents of ``dataset_description.json`` currently are an empty template (note that
we pipe the output of the ``cat`` command to `jq <https://stedolan.github.io/jq/>`_,
a command line JSON processor that prettifies the output):

.. runrecord:: _examples/DL-101-151-105
   :language: console
   :workdir: dl-101/mystudy

   $ cat dataset_description.json | jq

Most of the fields will be straightforward to answer for you, and you should input
values for them into the empty strings or JSON lists the template provides.
You can either do this with an editor of your choice and save your changes with
:command:`datalad save` afterwards, or use ``hirni``\s webUI for assistance. If
you do it by hand and are not too familiar with JSON, be mindful to not create
invalid JSON by accidentally removing commas or forgetting about quotes!
There are helpful commandline
tools such as `jq <https://stedolan.github.io/jq/>`_ or webtools such as
`JSONlint <https://jsonlint.com/>`_ to help verify whether your files stayed valid.

.. findoutmore:: Using the webUI

   Let's take a quick look into how web assisted curation works. In order to fill
   out ``dataset_description.json``, start the web tool [#f4]_ with

   .. code-block:: bash

      $ datalad webapp --dataset . hirni

   You will be able to open ``http://127.0.0.1:5000/`` in a browser and see the
   following interface:

   .. figure:: ../artwork/src/hirni_sc1.png

   By clicking "Edit Study Metadata" you will be directed to a form with the
   same fields as ``dataset_description.json``. The text fields contain short
   help or example messages that you can replace. There is no need to fill in
   all fields at once -- leave fields with information that you don't yet have blank.
   You can return to this file later, and also edit existing contents.
   As soon as you are done filling out as much as you can or like, click the
   ``Save to dataset`` button at the end of the form.

   .. figure:: ../artwork/src/hirni_sc2.png

   Internally, this process writes your input to the ``dataset_description.json``
   file and concludes with a ``datalad save``, using the commit message
   ``"[HIRNI] Add specification snippet for: dataset_description.json"``

Importing DICOMs
""""""""""""""""

As soon as the first acquisition exists, DICOM data can be imported into the study
dataset. A DICOM acquisition import takes the following structure in your study dataset:

.. code-block:: bash
   :emphasize-lines: 1-7

   ├── acquisition1             # subdirectory
   │   └── dicoms               # subdataset
   │       └──  ...
   │   ├── studyspec.json
   │   ├── protocol.pdf
   │   └── physio
   │       └──  ...
   ├── code
   │   └── hirni-toolbox
   │       ├── analyses
   │       ├── converters
   │       ├── data-retrieval
   │       ├── postprocessing
   │       └── procedures
   ├── dataset_description.json
   ├── README
   └── studyspec.json

Let's decipher that:

- There will be one subdirectory per acquisition tarball. In the above example,
  there is one acquisition, and it is represented in the subdirectory ``acquisition1``.
- Inside of this subdirectory, there will be at minimum a ``studyspec.json`` file
  with the specifications of this acquisition, and a ``dicoms/`` subdataset that
  contains the extracted DICOMs of the acquisition.
- Optionally, other study relevant data such as the
  acquisition protocol or physiological data can be added.

.. index:: ! datalad command; hirni-import-dcm

This structure is automatically created if ``hirni`` is pointed to the DICOM
tarball with the :command:`datalad hirni-import-dcm`
command. This command needs a path or url to a DICOM tarball of one acquisition,
and an acquisition ID that will become the subdirectory name. In the file hierarchy
above, this acquisition ID was ``acquisition1``.
Additionally, the command can take a few specifications that will be relevant for
conversion as optional command line arguments: You can specify the subject identifier as used
during scanning with the ``--subject`` option, and an anonymized ID (such as ``001``)
that this subject should be identified with in the final BIDS dataset
with the ``--anon-subject`` option.
Note that *some* of this information can be inferred from DICOM headers if
it was correctly filled in during scanning, for example the acquisition ID and
the subject identifier.

To import the first acquisition of the first subject, you run a command similar
to this one inside of your study dataset::

   $ datalad hirni-import-dcm --anon-subject 001 path/to/DICOMs.tar.gz acquisition1

Throughout your data acquisition, or -- if everything is already acquired -- in
one go afterwards, you import all acquisitions of the study into your study
dataset.

Semi-automatic metadata extraction and conversion specification
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

Ultimately, the :command:`datalad hirni-import-dcm` serves only one goal:
Collect all relevant information from the DICOM headers to enable the conversion.
This information is stored as aggregated metadata from the subdataset
in ``.datalad/metadata`` [#f5]_, and as acquisition-wise specifications in
``<acquisition-ID>/studyspec.json``.
You can find out more about the details of this process below.

.. findoutmore:: Internals of this command

   :command:`datalad hirni-import-dcm` executes a number of commands in order to
   prepare the conversion of the study dataset.

   Ultimately, DICOMs are only extracted from the tarball archive to access and
   query their headers' metadata. For some of this metadata extraction, the ``dicom``
   :term:`extractor` of ``datalad-neuroimaging`` is used. Together with
   ``datalad-metalad``, this extractor can record metadata from standard DICOM files,
   and, among other things, group together DICOM files that belong to an image series,
   i.e., a single sequence within the acquisition.
   The resulting metadata is *aggregated* into the study dataset [#f5]_.

   Beyond that, ``hirni``\s own metadata command :command:`hirni-dicom2spec` is
   executed and prepopulated an acquisition-specific ``studyspec.json`` file
   inside of the acquisition subdirectory. This file contains specifications
   for later structuring and naming
   of converted data to BIDS-compliant structures, such as the task name.
   Unlike the metadata mentioned above, metadata in ``studyspec.json`` files can and
   should be manually edited to insert custom or yet missing conversion-relevant
   information later. During DICOM import this file is prepopulated with relevant
   fields, and prefills fields with available metadata where available,
   for example about the location and version of the files and any BIDS-relevant
   information that is available from the DICOM headers.

   All of this metadata extraction is therefore crucial for the upcoming conversion.
   and the study dataset will receive a large chunk of the DICOM metadata
   required for the conversion directly at the time of the DICOM import.

   The basis of the import and data handling is a :command:`datalad add-archive-content`
   (:manpage:`datalad-add-archive-content` manual) command. This command -- together
   with the way the data import is performed -- helps to save disk space in the
   study dataset: The ``tar.gz`` archive is saved in its compressed form into a
   :term:`branch` of the subdataset, and is extracted into the ``master`` branch
   of the dataset. From the DICOM files in ``master``, metadata can be extracted.
   With :command:`datalad add-archive-content`, the archive in the dataset branch
   is treated like a :term:`special remote`: On demand, individual files from
   ``master`` could be re-retrieved from the tarball. Therefore, the archive in
   the seperate branch is essentially a compressed DICOM storage.
   Because this ensures one available source for the data, the (larger) file
   content of the extracted DICOMs in ``master`` can be dropped
   right after metadata aggregation. Thus, instead of large, extracted DICOMs,
   the subdatasets only contains the compressed tarball, while ensuring that
   DICOM files can nevertheless be reobtained from the archive on demand.

However, not all relevant information can be inferred from the DICOM headers.
Sometimes, DICOM headers contain false information, too, or acquisitions contain
image series from failed MRI sequences that should not be converted. And while
``hirni``\s conversion *can* incorporate special procedures such as anonymizing
subject identifiers or defacing anatomical images, it still needs a human decision
to do so. For these and similar cases that require human input, manual
specifications are necessary.

.. findoutmore:: Why anonymizing measures should be done during the conversion

   Basic ethical practices in science and the General Data Protection Regulation
   (GDPR) of the European Union require that potentially identifying information
   of subjects' data needs to be anonymized to protect the identity and privacy
   rights of research participants.

   MRI data contains a lot of identifiable data. DICOM headers can contain names
   or birth dates of participants, and even subject identifiers can allow
   guestimates about the participants.
   Actual raw DICOM data from neuroimaging studies also contains faces -- a
   *very* identifying piece of information about a person. None of this information
   is therefore allowed be distributed to people that did not conduct the original
   study: In order to comply to ethical and privacy requirements, datasets with
   neuroimaging data need to be anonymized. Usually, this requires anonymous
   subject identifiers (ascending zero-padded integers for simultaneous
   BIDS-compliance, for example) and "face-stripping", i.e., defacing steps that
   erase the facial parts of the MRI images. But if this is only done after the
   initial conversion to an analysis dataset, version control features lead to
   potential privacy breaches: If unanonymized data is once part of the resulting
   BIDS dataset, it stays in its history (unless someone aggressively rewrites
   the history -- which is discouraged as it threatens provenance).
   If the conversion takes care of anonymizing data right away,
   though, only anonymized files are saved in the BIDS-compliant analysis dataset.
   This way, the advantages of provenance and version control can be achieved
   without sacrificing participants privacy. Do note though that this is only
   true if the raw DICOM data is **not** made available -- this should never
   be the case though, and would compromise anonymity also in non-version control
   setups.

Durign DICOM import, the command ``hirni-dicom2spec`` generated so called
*specification snippets* from DICOM metadata and stored them in the
``studyspec.json`` file.
A specification snippet defines a list of procedures and how exactly they are called.

Specifications are fundamental to ``hirni`` and indispensable for a successful
conversion. But they are also quite hard to get a good grasp on. This
findoutmore will shed some light on them.

.. findoutmore:: On specifications

   Specifications are defined in ``studyspec.json`` files. These files contain
   specification snippets as one line of JSON stream.

   .. findoutmore:: Some fundamentals of JSON and JSON stream

      JSON (JavaScript Object Notation) is easy to read when it is formatted
      nicely (for example with `jq <https://stedolan.github.io/jq/>`_): In JSON,
      data is stored in ``key-value`` pairs (such as ``{"name":"Joanna"}``), and
      separated by commas. Curly braces (``{}``)
      hold *objects* (dictionaries), such as

      .. code-block:: JSON

         {
          "person": {
             "name": "Joanna",
             "role": "data scientist"
             }
          }

      and square brackets (``[]``) hold *arrays* (lists), as in

      .. code-block:: JSON

         {
          "employees":[
           {
            "name": "Joanna",
            "role": "data scientist"
            },
            {
             "name": "Anna",
             "role": "software developer"
            },
            {
              "name": "Peter",
              "role": "marketing analyst"
            }
           ]
         }

      JSON has the inconvenient side effect that a single mistake in the document
      (e.g., a missing comma, or misplaced bracket) invalidates the whole document.
      Therefore, specifications are written in JSON streaming format.
      In JSON stream, the JSON essentially contains fewer linebreaks. Each
      line contains one valid JSON dictionary. This format allows per-line processing,
      and guarantees that all valid lines can be processed, even if the document
      contains some invalid lines. Here is an example of one specification snippet
      (this one belongs to the DICOMs of an acquisition). It is represented as
      a nested JSON dictionary and is one long line of text::

         $ {"anon-subject":{"approved":false,"value":"001"},"bids-acquisition":{"approved":false,"value":null},"bids-contrast-enhancement":{"approved":false,"value":null},"bids-direction":{"approved":false,"value":null},"bids-echo":{"approved":false,"value":null},"bids-modality":{"approved":false,"value":"bold"},"bids-reconstruction-algorithm":{"approved":false,"value":null},"bids-run":{"approved":false,"value":"01"},"bids-session":{"approved":false,"value":null},"bids-task":{"approved":false,"value":"oneback"},"comment":{"approved":false,"value":""},"dataset-id":"2f2a44d8-7271-11ea-861d-f9fd9dd57046","dataset-refcommit":"64644f46a471424df817dd5001f72b223c6f7a33","description":{"approved":false,"value":"func_task-oneback_run-1"},"id":{"approved":false,"value":401},"location":"dicoms","procedures":[{"on-anonymize":{"approved":false,"value":false},"procedure-call":{"approved":false,"value":null},"procedure-name":{"approved":false,"value":"hirni-dicom-converter"}}],"subject":{"approved":false,"value":"02"},"type":"dicomseries:all"}


Conversion
""""""""""


In order to transform DICOMs to NifTi files, DICOM files need to be sorted into
series, i.e., sets of DICOM images acquired together. In many cases, these
series will be available as DICOM tarballs from the MRI scanner. Depending
on the length of data acquisition, such a tarball can contain thousands of
DICOM files for a single subject. Specialized
tools such as `dcm2niix <https://www.nitrc.org/plugins/mwiki/index.php/dcm2nii:MainPage>`_
can extract the header metadata, sort images into volumes, and stack the images
of a series to create the NifTi image.


.. rubric:: Footnotes

.. [#f1] To re-read about DataLad's run-procedures, check out section :ref:`procedures`.

.. [#f2] A ``dataset_description.json`` file exists because this file is
         `required <https://bids-specification.readthedocs.io/en/derivatives/03-modality-agnostic-files.html#dataset_descriptionjson>`_
         for valid BIDS datasets. Even if you are just in the planning phase of your
         study, you will still be able to already populate the template with you study's
         information.

.. [#f3] To re-read on capturing software environments as containers in datasets,
         go back to section :ref:`containersrun`.

.. [#f4] The web tool relies on ``datalad-webapp``. It is another DataLad extension
         that is automatically installed
         as a dependency of ``datalad-hirni``. Please note: Should you install
         ``datalad-hirni`` in its development version directly from within its Git
         repository, relevant resources for the webapp need to be retrieved by hand.
         To do this, run ``git annex get`` in ``hirni``\s Git repository.

.. [#f5] The chapter

         .. todo::

            Write metalad chapter

         introduces DataLad's metadata capabilities and demonstrates the metadata
         aggregation process in detail.